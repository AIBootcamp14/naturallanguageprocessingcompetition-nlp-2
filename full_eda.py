import pandas as pd
from transformers import AutoTokenizer
import os
import numpy as np # Numpyë¥¼ ì¶”ê°€ë¡œ ì‚¬ìš©í•´ì„œ ì•ˆì •ì ì¸ ê³„ì‚°ì„ í•  ê±°ì•¼
from pprint import pprint
from typing import Dict, Any

# 1. Configì™€ Tokenizer ë¡œë“œ (ì´ì „ì— ì‚¬ìš©ëœ ê²ƒê³¼ ë™ì¼)
tokenizer = AutoTokenizer.from_pretrained("digit82/kobart-summarization")

# 2. ë°ì´í„° ë¡œë“œ ê²½ë¡œ ì„¤ì • (ì‚¬ìš©ì ê²½ë¡œì™€ ì¼ì¹˜í•˜ë„ë¡ ìˆ˜ì •: ./data/)
DATA_PATH = "./data/" 

def calculate_token_lengths(text_series: pd.Series, tokenizer: AutoTokenizer) -> Dict[str, Any]:
    """ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ Seriesì˜ í† í° ê¸¸ì´ë¥¼ ê³„ì‚°í•˜ê³  ì•ˆì •ì ì¸ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    
    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬: ë¶ˆí•„ìš”í•œ ì¤„ë°”ê¿ˆì´ë‚˜ íƒœê·¸ ì œê±°
    cleaned_texts = text_series.apply(
        lambda x: str(x).replace('\n', ' ').replace('<br>', ' ')
    )
    # í† í° ê¸¸ì´ ì¸¡ì •
    token_lengths = [len(tokenizer.tokenize(text)) for text in cleaned_texts]
    
    # âš ï¸ Numpyë¥¼ ì‚¬ìš©í•´ 95%ì™€ 99% ê°’ì„ ì•ˆì •ì ìœ¼ë¡œ ê³„ì‚°
    lengths = np.array(token_lengths)

    # ê¸¸ì´ê°€ 0ì¸ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ì˜ˆì™¸ ì²˜ë¦¬
    if len(lengths) == 0:
        return {
            'count': 0, 'mean': 0, 'std': 0, 'min': 0, 
            '50%': 0, '95%': 0, '99%': 0, 'max': 0
        }

    # í†µê³„ ê³„ì‚°
    stats = {
        'count': len(lengths),
        'mean': round(np.mean(lengths), 3),
        'std': round(np.std(lengths), 3),
        'min': int(np.min(lengths)),
        '50%': int(np.percentile(lengths, 50)), # ì¤‘ì•™ê°’
        '95%': int(np.percentile(lengths, 95)), # 95% ì§€ì 
        '99%': int(np.percentile(lengths, 99)), # 99% ì§€ì  (ê°€ì¥ ì¤‘ìš”)
        'max': int(np.max(lengths))
    }
    
    return stats

def run_full_eda(data_path: str):
    """Train, Dev, Test ë°ì´í„°ì…‹ì˜ ëŒ€í™” ë° ìš”ì•½ í† í° ê¸¸ì´ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
    print("âœ¨ ì „ì²´ ë°ì´í„°ì…‹ (Train, Dev, Test) í† í° ê¸¸ì´ EDA ì‹œì‘...")
    
    # íŒŒì¼ ê²½ë¡œê°€ 'notebooks/data/'ì— ìˆì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ ìƒëŒ€ ê²½ë¡œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
    # ì¼ë°˜ì ìœ¼ë¡œ ëŒ€íšŒ í™˜ê²½ì—ì„œëŠ” './data/'ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, í˜¹ì‹œ ë¬¸ì œê°€ ìƒê¸°ë©´ 'notebooks/data/'ë¡œ ìˆ˜ì •í•´ì•¼ í•  ìˆ˜ë„ ìˆì–´.
    
    datasets = {
        "train": os.path.join(data_path, 'train.csv'),
        "dev": os.path.join(data_path, 'dev.csv'),
        "test": os.path.join(data_path, 'test.csv')
    }
    
    all_stats = {}
    
    for name, file_path in datasets.items():
        if not os.path.exists(file_path):
            # íŒŒì¼ì´ ì—†ìœ¼ë©´, ë‹¤ë¥¸ ê²½ë¡œì¸ 'notebooks/data/'ë¥¼ ì‹œë„í•´ ë³¼ ìˆ˜ë„ ìˆì§€ë§Œ, 
            # ì¼ë‹¨ ì‚¬ìš©ì ê²½ë¡œì¸ './data/'ë¥¼ ì‚¬ìš©í•œë‹¤ê³  ê°€ì •í• ê²Œ.
            print(f"âš ï¸ íŒŒì¼ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {file_path}. ë°ì´í„° ê²½ë¡œë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”.")
            continue
            
        try:
            df = pd.read_csv(file_path)
            print(f"\n--- {name.upper()} ë°ì´í„°ì…‹ ({len(df)}ê±´) ë¶„ì„ ì¤‘... ---")

            # 1. Dialogue Length Analysis (ì¸ì½”ë” ê¸¸ì´ ê²°ì •)
            dialogue_stats = calculate_token_lengths(df['dialogue'], tokenizer)
            all_stats[f'{name}_dialogue'] = dialogue_stats
            
            # 2. Summary Length Analysis (ë””ì½”ë” ê¸¸ì´ ê²°ì •)
            if 'summary' in df.columns:
                summary_stats = calculate_token_lengths(df['summary'], tokenizer)
                all_stats[f'{name}_summary'] = summary_stats
            else:
                print(f"[{name.upper()}] ìš”ì•½(Summary) ì»¬ëŸ¼ì€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì— ì—†ìœ¼ë¯€ë¡œ ìƒëµí•©ë‹ˆë‹¤.")
        
        except Exception as e:
            print(f"âŒ {name.upper()} ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            
    print("\n=======================================================")
    print("âœ… ì „ì²´ ë°ì´í„°ì…‹ í† í° ê¸¸ì´ EDA ê²°ê³¼:")
    print("=======================================================")
    
    # ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥
    for key, stats in all_stats.items():
        print(f"\n[ğŸ“Š {key.upper()} í†µê³„]")
        pprint(stats)

# --- ë©”ì¸ ì‹¤í–‰ ---
if __name__ == "__main__":
    run_full_eda(DATA_PATH)
