{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6n-Ps2nkVsBb"
   },
   "source": [
    "# **ğŸ’ğŸ»ğŸ—¨ï¸ğŸ’ğŸ»â€â™‚ï¸ëŒ€í™” ìš”ì•½ Baseline code**\n",
    "> **Dialogue Summarization** ê²½ì§„ëŒ€íšŒì— ì˜¤ì‹  ì—¬ëŸ¬ë¶„ í™˜ì˜í•©ë‹ˆë‹¤! ğŸ‰    \n",
    "> ë³¸ ëŒ€íšŒì—ì„œëŠ” ìµœì†Œ 2ëª…ì—ì„œ ìµœëŒ€ 7ëª…ì´ ë“±ì¥í•˜ì—¬ ë‚˜ëˆ„ëŠ” ëŒ€í™”ë¥¼ ìš”ì•½í•˜ëŠ” BART ê¸°ë°˜ ëª¨ë¸ì˜ baseline codeë¥¼ ì œê³µí•©ë‹ˆë‹¤.     \n",
    "> ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ì¼ìƒ ëŒ€í™”ì— ëŒ€í•œ ìš”ì•½ì„ íš¨ê³¼ì ìœ¼ë¡œ ìƒì„±í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ì–´ë´…ì‹œë‹¤!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNq_LylZa1ug"
   },
   "source": [
    "## âš™ï¸ ë°ì´í„° ë° í™˜ê²½ì„¤ì •"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjCiuI_V4glr"
   },
   "source": [
    "### 1) í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYqDF_-r2ToB"
   },
   "source": [
    "- í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•œ í›„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "zbZ7SU9P2TYN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import yaml\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from rouge import Rouge # ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\n",
    "\n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "from transformers import AutoTokenizer, BartForConditionalGeneration, BartConfig\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "import wandb # ëª¨ë¸ í•™ìŠµ ê³¼ì •ì„ ì†ì‰½ê²Œ Trackingí•˜ê³ , ì‹œê°í™”í•  ìˆ˜ ìˆëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ObEASD6Wj6pl"
   },
   "source": [
    "### 3) Configuration ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JUBm_6RqlYpV",
    "outputId": "4b1c8c44-c6a9-40f1-adbd-72d48f0c983b"
   },
   "outputs": [],
   "source": [
    "# ì €ì¥ëœ config íŒŒì¼ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "config_path = \"./config.yaml\"\n",
    "\n",
    "with open(config_path, \"r\") as file:\n",
    "    loaded_config = yaml.safe_load(file)\n",
    "\n",
    "# ë¶ˆëŸ¬ì˜¨ config íŒŒì¼ì˜ ì „ì²´ ë‚´ìš©ì„ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "# pprint(loaded_config)\n",
    "\n",
    "\n",
    "data_path = loaded_config['general']['data_path']\n",
    "train_df = pd.read_csv(os.path.join(data_path,'train.csv'))\n",
    "val_df = pd.read_csv(os.path.join(data_path,'dev.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í‰ê·  ê¸¸ì´: 84.46447780364454 ìµœì†Œ: 17 ìµœëŒ€: 508\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(d.split()) for d in train_df['dialogue']]\n",
    "print(\"í‰ê·  ê¸¸ì´:\", np.mean(lengths), \"ìµœì†Œ:\", np.min(lengths), \"ìµœëŒ€:\", np.max(lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IIaIrpH4kWo"
   },
   "source": [
    "## 1. ë°ì´í„° ê°€ê³µ ë° ë°ì´í„°ì…‹ í´ë˜ìŠ¤ êµ¬ì¶•\n",
    "- csv file ì„ ë¶ˆëŸ¬ì™€ì„œ encoder ì™€ decoderì˜ ì…ë ¥í˜•íƒœë¡œ ê°€ê³µí•´ì¤ë‹ˆë‹¤.\n",
    "- ê°€ê³µëœ ë°ì´í„°ë¥¼ torch dataset class ë¡œ êµ¬ì¶•í•˜ì—¬ ëª¨ë¸ì— ì…ë ¥ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë§Œë“­ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "oWPawUUflwHa"
   },
   "outputs": [],
   "source": [
    "# ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ í´ë˜ìŠ¤ë¡œ, ë°ì´í„°ì…‹ì„ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜í•˜ê³  ì¸ì½”ë”ì™€ ë””ì½”ë”ì˜ ì…ë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "class Preprocess:\n",
    "    def __init__(self,\n",
    "            bos_token: str,\n",
    "            eos_token: str,\n",
    "        ) -> None:\n",
    "\n",
    "        self.bos_token = bos_token\n",
    "        self.eos_token = eos_token\n",
    "    \n",
    "    @staticmethod    \n",
    "    def preprocess_data(df):\n",
    "        df = df.copy()\n",
    "        # fname ì •ë¦¬\n",
    "        df['fname'] = df['fname'].str.strip()\n",
    "        \n",
    "        # summary ì •ë¦¬\n",
    "        if 'summary' in df.columns:\n",
    "            df['summary'] = df['summary'].str.strip()\n",
    "            df['summary'] = df['summary'].str.replace(r'\\s+', ' ', regex=True)\n",
    "            df['summary'] = df['summary'].str.strip('\"').str.strip(\"'\")\n",
    "        \n",
    "        # dialogue ì •ë¦¬\n",
    "        if 'dialogue' in df.columns:\n",
    "            df['dialogue'] = df['dialogue'].str.strip()\n",
    "            df['dialogue'] = df['dialogue'].str.replace(r'\\s+', ' ', regex=True)\n",
    "            df['dialogue'] = df['dialogue'].str.strip('\"').str.strip(\"'\")\n",
    "        \n",
    "        return df\n",
    "                \n",
    "\n",
    "    @staticmethod\n",
    "    # ì‹¤í—˜ì— í•„ìš”í•œ ì»¬ëŸ¼ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "    def make_set_as_df(file_path, is_train = True):\n",
    "        if is_train:\n",
    "            df = pd.read_csv(file_path)\n",
    "            train_df = df[['fname','dialogue','summary']]\n",
    "            train_df = Preprocess.preprocess_data(train_df)\n",
    "            return train_df\n",
    "        else:\n",
    "            df = pd.read_csv(file_path)\n",
    "            test_df = df[['fname','dialogue']]\n",
    "            test_df = Preprocess.preprocess_data(test_df)\n",
    "            return test_df\n",
    "    \n",
    "    # token deletion ê¸°ë²• í™œìš©í•´ ì¦ê°•\n",
    "    def simple_safe_augmentation(self, dataset, special_tokens, sample_ratio=0.3, drop_prob=0.1, min_len=45, seed=42):\n",
    "        random.seed(seed)\n",
    "        new_data = {'dialogue': [], 'summary': []}\n",
    "\n",
    "        for _, row in dataset.iterrows():\n",
    "            dialogue = row['dialogue']\n",
    "            summary = row['summary']\n",
    "\n",
    "            # ì›ë³¸\n",
    "            new_data['dialogue'].append(dialogue)\n",
    "            new_data['summary'].append(summary)\n",
    "\n",
    "            # 30% í™•ë¥ ë¡œ ì¦ê°•\n",
    "            if random.random() < sample_ratio:\n",
    "                tokens = dialogue.split()\n",
    "                if len(tokens) > (min_len + 2):  # ìµœì†Œ ê¸¸ì´ë³´ë‹¤ ì—¬ìœ  ìˆì„ ë•Œë§Œ\n",
    "                    kept = []\n",
    "                    for t in tokens:\n",
    "                        if t in special_tokens:   # ìŠ¤í˜ì…œ í† í° ë¬´ì¡°ê±´ ë³´ì¡´\n",
    "                            kept.append(t)\n",
    "                        elif random.random() > drop_prob:  # 10% ë“œë¡­ì•„ì›ƒ\n",
    "                            kept.append(t)\n",
    "                    if len(kept) >= min_len:\n",
    "                        new_data['dialogue'].append(\" \".join(kept))\n",
    "                        new_data['summary'].append(summary)\n",
    "\n",
    "        out = pd.DataFrame(new_data)\n",
    "\n",
    "        return out\n",
    "    \n",
    "            \n",
    "            \n",
    "    # BART ëª¨ë¸ì˜ ì…ë ¥, ì¶œë ¥ í˜•íƒœë¥¼ ë§ì¶”ê¸° ìœ„í•´ ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.\n",
    "    def make_input(self, dataset,is_test = False):\n",
    "        if is_test:\n",
    "            encoder_input = dataset['dialogue']\n",
    "            decoder_input = [self.bos_token] * len(dataset['dialogue'])\n",
    "            return encoder_input.tolist(), list(decoder_input)\n",
    "        else:\n",
    "            encoder_input = dataset['dialogue']\n",
    "            decoder_input = dataset['summary'].apply(lambda x : self.bos_token + str(x)) # Ground truthë¥¼ ë””ì½”ë”ì˜ inputìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•©ë‹ˆë‹¤.\n",
    "            decoder_output = dataset['summary'].apply(lambda x : str(x) + self.eos_token)\n",
    "            return encoder_input.tolist(), decoder_input.tolist(), decoder_output.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "6GDvodoF8sED"
   },
   "outputs": [],
   "source": [
    "# Trainì— ì‚¬ìš©ë˜ëŠ” Dataset í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "class DatasetForTrain(Dataset):\n",
    "    def __init__(self, encoder_input, decoder_input, labels, len):\n",
    "        self.encoder_input = encoder_input\n",
    "        self.decoder_input = decoder_input\n",
    "        self.labels = labels\n",
    "        self.len = len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()} # item[input_ids], item[attention_mask]\n",
    "        item2 = {key: val[idx].clone().detach() for key, val in self.decoder_input.items()} # item2[input_ids], item2[attention_mask]\n",
    "        item2['decoder_input_ids'] = item2['input_ids']\n",
    "        item2['decoder_attention_mask'] = item2['attention_mask']\n",
    "        item2.pop('input_ids')\n",
    "        item2.pop('attention_mask')\n",
    "        item.update(item2) #item[input_ids], item[attention_mask] item[decoder_input_ids], item[decoder_attention_mask]\n",
    "        item['labels'] = self.labels['input_ids'][idx] #item[input_ids], item[attention_mask] item[decoder_input_ids], item[decoder_attention_mask], item[labels]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "# Validationì— ì‚¬ìš©ë˜ëŠ” Dataset í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "class DatasetForVal(Dataset):\n",
    "    def __init__(self, encoder_input, decoder_input, labels, len):\n",
    "        self.encoder_input = encoder_input\n",
    "        self.decoder_input = decoder_input\n",
    "        self.labels = labels\n",
    "        self.len = len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()} # item[input_ids], item[attention_mask]\n",
    "        item2 = {key: val[idx].clone().detach() for key, val in self.decoder_input.items()} # item2[input_ids], item2[attention_mask]\n",
    "        item2['decoder_input_ids'] = item2['input_ids']\n",
    "        item2['decoder_attention_mask'] = item2['attention_mask']\n",
    "        item2.pop('input_ids')\n",
    "        item2.pop('attention_mask')\n",
    "        item.update(item2) #item[input_ids], item[attention_mask] item[decoder_input_ids], item[decoder_attention_mask]\n",
    "        item['labels'] = self.labels['input_ids'][idx] #item[input_ids], item[attention_mask] item[decoder_input_ids], item[decoder_attention_mask], item[labels]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "# Testì— ì‚¬ìš©ë˜ëŠ” Dataset í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "class DatasetForInference(Dataset):\n",
    "    def __init__(self, encoder_input, test_id, len):\n",
    "        self.encoder_input = encoder_input\n",
    "        self.test_id = test_id\n",
    "        self.len = len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}\n",
    "        item['ID'] = self.test_id[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "hT9z4vvS2CCb"
   },
   "outputs": [],
   "source": [
    "# tokenization ê³¼ì •ê¹Œì§€ ì§„í–‰ëœ ìµœì¢…ì ìœ¼ë¡œ ëª¨ë¸ì— ì…ë ¥ë  ë°ì´í„°ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "def prepare_train_dataset(config, preprocessor, data_path, tokenizer):\n",
    "    train_file_path = os.path.join(data_path,'train.csv')\n",
    "    val_file_path = os.path.join(data_path,'dev.csv')\n",
    "\n",
    "    # train, validationì— ëŒ€í•´ ê°ê° ë°ì´í„°í”„ë ˆì„ì„ êµ¬ì¶•í•©ë‹ˆë‹¤.\n",
    "    train_data = preprocessor.make_set_as_df(train_file_path)\n",
    "    val_data = preprocessor.make_set_as_df(val_file_path)\n",
    "\n",
    "    print('-'*150)\n",
    "    print(f'train_data:\\n {train_data[\"dialogue\"][0]}')\n",
    "    print(f'train_label:\\n {train_data[\"summary\"][0]}')\n",
    "\n",
    "    print('-'*150)\n",
    "    print(f'val_data:\\n {val_data[\"dialogue\"][0]}')\n",
    "    print(f'val_label:\\n {val_data[\"summary\"][0]}')\n",
    "    \n",
    "    SPECIAL_TOKENS = set(config['tokenizer']['special_tokens'])\n",
    "    train_data = preprocessor.simple_safe_augmentation(train_data, special_tokens=SPECIAL_TOKENS, sample_ratio=0.3, drop_prob=0.1, min_len=45, seed=42)\n",
    "\n",
    "    encoder_input_train , decoder_input_train, decoder_output_train = preprocessor.make_input(train_data)\n",
    "    encoder_input_val , decoder_input_val, decoder_output_val = preprocessor.make_input(val_data)\n",
    "    print('-'*10, 'Load data complete', '-'*10,)\n",
    "\n",
    "    tokenized_encoder_inputs = tokenizer(encoder_input_train, return_tensors=\"pt\", padding=True,\n",
    "                            add_special_tokens=True, truncation=True, max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False)\n",
    "    tokenized_decoder_inputs = tokenizer(decoder_input_train, return_tensors=\"pt\", padding=True,\n",
    "                        add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
    "    tokenized_decoder_ouputs = tokenizer(decoder_output_train, return_tensors=\"pt\", padding=True,\n",
    "                        add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
    "\n",
    "    train_inputs_dataset = DatasetForTrain(tokenized_encoder_inputs, tokenized_decoder_inputs, tokenized_decoder_ouputs,len(encoder_input_train))\n",
    "\n",
    "    val_tokenized_encoder_inputs = tokenizer(encoder_input_val, return_tensors=\"pt\", padding=True,\n",
    "                        add_special_tokens=True, truncation=True, max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False)\n",
    "    val_tokenized_decoder_inputs = tokenizer(decoder_input_val, return_tensors=\"pt\", padding=True,\n",
    "                        add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
    "    val_tokenized_decoder_ouputs = tokenizer(decoder_output_val, return_tensors=\"pt\", padding=True,\n",
    "                        add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
    "\n",
    "    val_inputs_dataset = DatasetForVal(val_tokenized_encoder_inputs, val_tokenized_decoder_inputs, val_tokenized_decoder_ouputs,len(encoder_input_val))\n",
    "\n",
    "    print('-'*10, 'Make dataset complete', '-'*10,)\n",
    "    return train_inputs_dataset, val_inputs_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5sKIJ5K5Pz1"
   },
   "source": [
    "## 2. Trainer ë° Trainingargs êµ¬ì¶•í•˜ê¸°\n",
    "- Huggingface ì˜ Trainer ì™€ Training argumentsë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ í•™ìŠµì„ ì¼ê´„ì ìœ¼ë¡œ ì²˜ë¦¬í•´ì£¼ëŠ” í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "aQk8ILcEeGNz"
   },
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì„±ëŠ¥ì— ëŒ€í•œ í‰ê°€ ì§€í‘œë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ë³¸ ëŒ€íšŒì—ì„œëŠ” ROUGE ì ìˆ˜ë¥¼ í†µí•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "def compute_metrics(config,tokenizer,pred):\n",
    "    rouge = Rouge()\n",
    "    predictions = pred.predictions\n",
    "    labels = pred.label_ids\n",
    "\n",
    "    predictions[predictions == -100] = tokenizer.pad_token_id\n",
    "    labels[labels == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, clean_up_tokenization_spaces=True)\n",
    "    labels = tokenizer.batch_decode(labels, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    # ì •í™•í•œ í‰ê°€ë¥¼ ìœ„í•´ ë¯¸ë¦¬ ì •ì˜ëœ ë¶ˆí•„ìš”í•œ ìƒì„±í† í°ë“¤ì„ ì œê±°í•©ë‹ˆë‹¤.\n",
    "    replaced_predictions = decoded_preds.copy()\n",
    "    replaced_labels = labels.copy()\n",
    "    remove_tokens = config['inference']['remove_tokens']\n",
    "    for token in remove_tokens:\n",
    "        replaced_predictions = [sentence.replace(token,\" \") for sentence in replaced_predictions]\n",
    "        replaced_labels = [sentence.replace(token,\" \") for sentence in replaced_labels]\n",
    "\n",
    "    print('-'*150)\n",
    "    print(f\"PRED: {replaced_predictions[0]}\")\n",
    "    print(f\"GOLD: {replaced_labels[0]}\")\n",
    "    print('-'*150)\n",
    "    print(f\"PRED: {replaced_predictions[1]}\")\n",
    "    print(f\"GOLD: {replaced_labels[1]}\")\n",
    "    print('-'*150)\n",
    "    print(f\"PRED: {replaced_predictions[2]}\")\n",
    "    print(f\"GOLD: {replaced_labels[2]}\")\n",
    "\n",
    "    # ìµœì¢…ì ì¸ ROUGE ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "    results = rouge.get_scores(replaced_predictions, replaced_labels,avg=True)\n",
    "\n",
    "    # ROUGE ì ìˆ˜ ì¤‘ F-1 scoreë¥¼ í†µí•´ í‰ê°€í•©ë‹ˆë‹¤.\n",
    "    result = {key: value[\"f\"] for key, value in results.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "RInkG8g-HjBi"
   },
   "outputs": [],
   "source": [
    "# í•™ìŠµì„ ìœ„í•œ trainer í´ë˜ìŠ¤ì™€ ë§¤ê°œë³€ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "def load_trainer_for_train(config,generate_model,tokenizer,train_inputs_dataset,val_inputs_dataset):\n",
    "    print('-'*10, 'Make training arguments', '-'*10,)\n",
    "    # set training args\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "                output_dir=config['general']['output_dir'], # model output directory\n",
    "                overwrite_output_dir=config['training']['overwrite_output_dir'],\n",
    "                num_train_epochs=config['training']['num_train_epochs'],  # total number of training epochs\n",
    "                learning_rate=config['training']['learning_rate'], # learning_rate\n",
    "                per_device_train_batch_size=config['training']['per_device_train_batch_size'], # batch size per device during training\n",
    "                per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],# batch size for evaluation\n",
    "                warmup_ratio=config['training']['warmup_ratio'],  # number of warmup steps for learning rate scheduler\n",
    "                weight_decay=config['training']['weight_decay'],  # strength of weight decay\n",
    "                lr_scheduler_type=config['training']['lr_scheduler_type'],\n",
    "                optim =config['training']['optim'],\n",
    "                gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],\n",
    "                eval_strategy=config['training']['evaluation_strategy'], # evaluation strategy to adopt during training\n",
    "                save_strategy =config['training']['save_strategy'],\n",
    "                save_total_limit=config['training']['save_total_limit'], # number of total save model.\n",
    "                fp16=config['training']['fp16'],\n",
    "                load_best_model_at_end=config['training']['load_best_model_at_end'], # ìµœì¢…ì ìœ¼ë¡œ ê°€ì¥ ë†’ì€ ì ìˆ˜ ì €ì¥\n",
    "                seed=config['training']['seed'],\n",
    "                logging_dir=config['training']['logging_dir'], # directory for storing logs\n",
    "                logging_strategy=config['training']['logging_strategy'],\n",
    "                predict_with_generate=config['training']['predict_with_generate'], #To use BLEU or ROUGE score\n",
    "                generation_max_length=config['training']['generation_max_length'],\n",
    "                do_train=config['training']['do_train'],\n",
    "                do_eval=config['training']['do_eval'],\n",
    "                metric_for_best_model=\"rouge-l\",\n",
    "                greater_is_better=True,  \n",
    "                report_to=[],\n",
    "                # report_to=config['training']['report_to'], # (ì„ íƒ) wandbë¥¼ ì‚¬ìš©í•  ë•Œ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "            )\n",
    "\n",
    "    # (ì„ íƒ) ëª¨ë¸ì˜ í•™ìŠµ ê³¼ì •ì„ ì¶”ì í•˜ëŠ” wandbë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì´ˆê¸°í™” í•´ì¤ë‹ˆë‹¤.\n",
    "    # wandb.init(\n",
    "    #     entity=config['wandb']['entity'],\n",
    "    #     project=config['wandb']['project'],\n",
    "    #     name=config['wandb']['name'],\n",
    "    #     settings=wandb.Settings(start_method=\"thread\", init_timeout=300)\n",
    "    # )\n",
    "\n",
    "    # # (ì„ íƒ) ëª¨ë¸ checkpointë¥¼ wandbì— ì €ì¥í•˜ë„ë¡ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "    # os.environ[\"WANDB_LOG_MODEL\"]=\"false\"\n",
    "    # os.environ[\"WANDB_WATCH\"]=\"false\"\n",
    "\n",
    "    # Validation lossê°€ ë” ì´ìƒ ê°œì„ ë˜ì§€ ì•Šì„ ë•Œ í•™ìŠµì„ ì¤‘ë‹¨ì‹œí‚¤ëŠ” EarlyStopping ê¸°ëŠ¥ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "    MyCallback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=config['training']['early_stopping_patience'],\n",
    "        early_stopping_threshold=config['training']['early_stopping_threshold']\n",
    "    )\n",
    "    print('-'*10, 'Make training arguments complete', '-'*10,)\n",
    "    print('-'*10, 'Make trainer', '-'*10,)\n",
    "\n",
    "    # Trainer í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=generate_model, # ì‚¬ìš©ìê°€ ì‚¬ì „ í•™ìŠµí•˜ê¸° ìœ„í•´ ì‚¬ìš©í•  ëª¨ë¸ì„ ì…ë ¥í•©ë‹ˆë‹¤.\n",
    "        args=training_args,\n",
    "        train_dataset=train_inputs_dataset,\n",
    "        eval_dataset=val_inputs_dataset,\n",
    "        compute_metrics = lambda pred: compute_metrics(config,tokenizer, pred),\n",
    "        callbacks = [MyCallback]\n",
    "    )\n",
    "    print('-'*10, 'Make trainer complete', '-'*10,)\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "KKWHe8dE5fSx"
   },
   "outputs": [],
   "source": [
    "# í•™ìŠµì„ ìœ„í•œ tokenizerì™€ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "def load_tokenizer_and_model_for_train(config,device):\n",
    "    print('-'*10, 'Load tokenizer & model', '-'*10,)\n",
    "    print('-'*10, f'Model Name : {config[\"general\"][\"model_name\"]}', '-'*10,)\n",
    "    model_name = config['general']['model_name']\n",
    "    bart_config = BartConfig().from_pretrained(model_name)\n",
    "    bart_config.tie_word_embeddings = True\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    generate_model = BartForConditionalGeneration.from_pretrained(config['general']['model_name'],config=bart_config)\n",
    "\n",
    "    special_tokens_dict={'additional_special_tokens':config['tokenizer']['special_tokens']}\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "    generate_model.resize_token_embeddings(len(tokenizer)) # ì‚¬ì „ì— special tokenì„ ì¶”ê°€í–ˆìœ¼ë¯€ë¡œ ì¬êµ¬ì„± í•´ì¤ë‹ˆë‹¤.\n",
    "    generate_model.to(device)\n",
    "    # print(generate_model.config)\n",
    "\n",
    "    print('-'*10, 'Load tokenizer & model complete', '-'*10,)\n",
    "    return generate_model , tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvutzKQYvQgl"
   },
   "source": [
    "## 3. ëª¨ë¸ í•™ìŠµí•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImZUb-BC42J-"
   },
   "source": [
    "- ì•ì—ì„œ êµ¬ì¶•í•œ í´ë˜ìŠ¤ ë° í•¨ìˆ˜ë¥¼ í™œìš©í•˜ì—¬ í•™ìŠµ ì§„í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "qnA96wmR44is"
   },
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    # ì‚¬ìš©í•  deviceë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available()  else 'cpu')\n",
    "    print('-'*10, f'device : {device}', '-'*10,)\n",
    "    print(torch.__version__)\n",
    "\n",
    "    # ì‚¬ìš©í•  ëª¨ë¸ê³¼ tokenizerë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "    generate_model , tokenizer = load_tokenizer_and_model_for_train(config,device)\n",
    "    print('-'*10,\"tokenizer special tokens : \",tokenizer.special_tokens_map,'-'*10)\n",
    "\n",
    "    # í•™ìŠµì— ì‚¬ìš©í•  ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "    preprocessor = Preprocess(config['tokenizer']['bos_token'], config['tokenizer']['eos_token']) # decoder_start_token: str, eos_token: str\n",
    "    data_path = config['general']['data_path']\n",
    "    train_inputs_dataset, val_inputs_dataset = prepare_train_dataset(config,preprocessor, data_path, tokenizer)\n",
    "\n",
    "    # Trainer í´ë˜ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "    trainer = load_trainer_for_train(config, generate_model,tokenizer,train_inputs_dataset,val_inputs_dataset)\n",
    "    trainer.train()   # ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.\n",
    "\n",
    "    # (ì„ íƒ) ëª¨ë¸ í•™ìŠµì´ ì™„ë£Œëœ í›„ wandbë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.\n",
    "    # wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/nlp/lib/python3.10/site-packages/torch/__init__.py:1125: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  return isinstance(obj, torch.Tensor)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.02 GB\n",
      "Reserved: 0.04 GB\n",
      "Total: 23.69 GB\n",
      "Free: 23.65 GB\n",
      "free=23.33 GB / total=23.69 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# GPU ë©”ëª¨ë¦¬ ì™„ì „ ì´ˆê¸°í™”\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# ëª¨ë“  CUDA í…ì„œ ì‚­ì œ\n",
    "for obj in gc.get_objects():\n",
    "    try:\n",
    "        if torch.is_tensor(obj) and obj.is_cuda:\n",
    "            del obj\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 1. í˜„ì¬ í• ë‹¹ëœ ë©”ëª¨ë¦¬ (PyTorchê°€ ì‚¬ìš© ì¤‘)\n",
    "allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "print(f\"Allocated: {allocated:.2f} GB\")  # ì‚¬ìš© ì¤‘ì¸ ë©”ëª¨ë¦¬\n",
    "\n",
    "# 2. ì˜ˆì•½ëœ ë©”ëª¨ë¦¬ (PyTorchê°€ í™•ë³´í•´ë‘” ë©”ëª¨ë¦¬)\n",
    "reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "print(f\"Reserved: {reserved:.2f} GB\")    # ìºì‹œ í¬í•¨\n",
    "\n",
    "# 3. ì „ì²´ GPU ë©”ëª¨ë¦¬\n",
    "total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "print(f\"Total: {total:.2f} GB\")          # GPU ì „ì²´ ìš©ëŸ‰\n",
    "\n",
    "# 4. ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬ (ëŒ€ëµ)\n",
    "free = total - reserved\n",
    "print(f\"Free: {free:.2f} GB\")\n",
    "\n",
    "# 5. ì„œë²„ ìì²´ì˜ ì‹¤ì œ ë‚¨ì€ ë©”ëª¨ë¦¬\n",
    "free, total = torch.cuda.mem_get_info()  # ë°”ì´íŠ¸ ë‹¨ìœ„, \"ë””ë°”ì´ìŠ¤\" ì”ì—¬\n",
    "print(f\"free={free/1024**3:.2f} GB / total={total/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1DMS60wL-Dhv",
    "outputId": "cbb6aba7-18ff-4d12-b9e7-2a2ef31d94d8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- device : cuda:0 ----------\n",
      "2.8.0+cu128\n",
      "---------- Load tokenizer & model ----------\n",
      "---------- Model Name : digit82/kobart-summarization ----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloaded_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 8\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# ì‚¬ìš©í•  ëª¨ë¸ê³¼ tokenizerë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m generate_model , tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mload_tokenizer_and_model_for_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer special tokens : \u001b[39m\u001b[38;5;124m\"\u001b[39m,tokenizer\u001b[38;5;241m.\u001b[39mspecial_tokens_map,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# í•™ìŠµì— ì‚¬ìš©í•  ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[27], line 8\u001b[0m, in \u001b[0;36mload_tokenizer_and_model_for_train\u001b[0;34m(config, device)\u001b[0m\n\u001b[1;32m      6\u001b[0m bart_config \u001b[38;5;241m=\u001b[39m BartConfig()\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m      7\u001b[0m bart_config\u001b[38;5;241m.\u001b[39mtie_word_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m generate_model \u001b[38;5;241m=\u001b[39m BartForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeneral\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m'\u001b[39m],config\u001b[38;5;241m=\u001b[39mbart_config)\n\u001b[1;32m     11\u001b[0m special_tokens_dict\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madditional_special_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m:config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecial_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:1125\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1122\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1123\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1124\u001b[0m         )\n\u001b[0;32m-> 1125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2012\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2008\u001b[0m             vocab_files[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_template_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2009\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHAT_TEMPLATE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate_file\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2010\u001b[0m             )\n\u001b[1;32m   2011\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2012\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m template \u001b[38;5;129;01min\u001b[39;00m \u001b[43mlist_repo_templates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2017\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   2018\u001b[0m         template \u001b[38;5;241m=\u001b[39m template\u001b[38;5;241m.\u001b[39mremovesuffix(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jinja\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2019\u001b[0m         vocab_files[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_template_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHAT_TEMPLATE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jinja\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/transformers/utils/hub.py:167\u001b[0m, in \u001b[0;36mlist_repo_templates\u001b[0;34m(repo_id, local_files_only, revision, cache_dir)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m local_files_only:\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    168\u001b[0m             entry\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mremoveprefix(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHAT_TEMPLATE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    169\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m list_repo_tree(\n\u001b[1;32m    170\u001b[0m                 repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[1;32m    171\u001b[0m                 revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    172\u001b[0m                 path_in_repo\u001b[38;5;241m=\u001b[39mCHAT_TEMPLATE_DIR,\n\u001b[1;32m    173\u001b[0m                 recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    174\u001b[0m             )\n\u001b[1;32m    175\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jinja\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    176\u001b[0m         ]\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (GatedRepoError, RepositoryNotFoundError, RevisionNotFoundError):\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# valid errors => do not catch\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/transformers/utils/hub.py:167\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m local_files_only:\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m    168\u001b[0m             entry\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mremoveprefix(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHAT_TEMPLATE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    169\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m list_repo_tree(\n\u001b[1;32m    170\u001b[0m                 repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[1;32m    171\u001b[0m                 revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    172\u001b[0m                 path_in_repo\u001b[38;5;241m=\u001b[39mCHAT_TEMPLATE_DIR,\n\u001b[1;32m    173\u001b[0m                 recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    174\u001b[0m             )\n\u001b[1;32m    175\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jinja\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    176\u001b[0m         ]\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (GatedRepoError, RepositoryNotFoundError, RevisionNotFoundError):\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# valid errors => do not catch\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/huggingface_hub/hf_api.py:3188\u001b[0m, in \u001b[0;36mHfApi.list_repo_tree\u001b[0;34m(self, repo_id, path_in_repo, recursive, expand, revision, repo_type, token)\u001b[0m\n\u001b[1;32m   3186\u001b[0m encoded_path_in_repo \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m quote(path_in_repo, safe\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m path_in_repo \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3187\u001b[0m tree_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/tree/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mencoded_path_in_repo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 3188\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path_info \u001b[38;5;129;01min\u001b[39;00m paginate(path\u001b[38;5;241m=\u001b[39mtree_url, headers\u001b[38;5;241m=\u001b[39mheaders, params\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecursive\u001b[39m\u001b[38;5;124m\"\u001b[39m: recursive, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpand\u001b[39m\u001b[38;5;124m\"\u001b[39m: expand}):\n\u001b[1;32m   3189\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m (RepoFile(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpath_info) \u001b[38;5;28;01mif\u001b[39;00m path_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m RepoFolder(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpath_info))\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/huggingface_hub/utils/_pagination.py:36\u001b[0m, in \u001b[0;36mpaginate\u001b[0;34m(path, params, headers)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fetch a list of models/datasets/spaces and paginate through results.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03mThis is using the same \"Link\" header format as GitHub.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m- https://docs.github.com/en/rest/guides/traversing-with-pagination#link-header\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     35\u001b[0m session \u001b[38;5;241m=\u001b[39m get_session()\n\u001b[0;32m---> 36\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m r\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/requests/sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[0;34m(self, url, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \n\u001b[1;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:95\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_curlify(request)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     97\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/requests/adapters.py:644\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    641\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 644\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/urllib3/connectionpool.py:534\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 534\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/site-packages/urllib3/connection.py:565\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    562\u001b[0m _shutdown \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    568\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/conda/envs/nlp/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main(loaded_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFtWqowCGzEc"
   },
   "source": [
    "## 4. ëª¨ë¸ ì¶”ë¡ í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒˆë¡­ê²Œ parameter ì„¤ì •í•œ config íŒŒì¼ì„ ë‹¤ì‹œ í•œë²ˆ ë¶ˆëŸ¬ì˜¤ê¸°!\n",
    "config_path = \"./config.yaml\"\n",
    "\n",
    "with open(config_path, \"r\") as file:\n",
    "    loaded_config = yaml.safe_load(file)\n",
    "\n",
    "# ì´ê³³ì— ë‚´ê°€ ì‚¬ìš©í•  wandb config ì„¤ì •\n",
    "loaded_config['inference']['ckt_path'] = \"/workspace/NLP_Dialogue_Summarization/output/model/checkpoint-6200\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFGul3-rSscf"
   },
   "source": [
    "- test dataë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "lV1Do7nlTylG"
   },
   "outputs": [],
   "source": [
    "# tokenization ê³¼ì •ê¹Œì§€ ì§„í–‰ëœ ìµœì¢…ì ìœ¼ë¡œ ëª¨ë¸ì— ì…ë ¥ë  ë°ì´í„°ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "def prepare_test_dataset(config,preprocessor, tokenizer):\n",
    "\n",
    "    test_file_path = os.path.join(config['general']['data_path'],'test.csv')\n",
    "\n",
    "    test_data = preprocessor.make_set_as_df(test_file_path,is_train=False)\n",
    "    test_id = test_data['fname']\n",
    "\n",
    "    print('-'*150)\n",
    "    print(f'test_data:\\n{test_data[\"dialogue\"][0]}')\n",
    "    print('-'*150)\n",
    "\n",
    "    encoder_input_test , decoder_input_test = preprocessor.make_input(test_data,is_test=True)\n",
    "    print('-'*10, 'Load data complete', '-'*10,)\n",
    "\n",
    "    test_tokenized_encoder_inputs = tokenizer(encoder_input_test, return_tensors=\"pt\", padding=True,\n",
    "                    add_special_tokens=True, truncation=True, max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False,)\n",
    "    test_tokenized_decoder_inputs = tokenizer(decoder_input_test, return_tensors=\"pt\", padding=True,\n",
    "                    add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False,)\n",
    "\n",
    "    test_encoder_inputs_dataset = DatasetForInference(test_tokenized_encoder_inputs, test_id, len(encoder_input_test))\n",
    "    print('-'*10, 'Make dataset complete', '-'*10,)\n",
    "\n",
    "    return test_data, test_encoder_inputs_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "eb49bLULT3aS"
   },
   "outputs": [],
   "source": [
    "# ì¶”ë¡ ì„ ìœ„í•œ tokenizerì™€ í•™ìŠµì‹œí‚¨ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n",
    "def load_tokenizer_and_model_for_test(config,device):\n",
    "    print('-'*10, 'Load tokenizer & model', '-'*10,)\n",
    "\n",
    "    model_name = config['general']['model_name']\n",
    "    ckt_path = config['inference']['ckt_path']\n",
    "    print('-'*10, f'Model Name : {model_name}', '-'*10,)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    special_tokens_dict = {'additional_special_tokens': config['tokenizer']['special_tokens']}\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "    generate_model = BartForConditionalGeneration.from_pretrained(ckt_path)\n",
    "    generate_model.resize_token_embeddings(len(tokenizer))\n",
    "    generate_model.to(device)\n",
    "    print('-'*10, 'Load tokenizer & model complete', '-'*10,)\n",
    "\n",
    "    return generate_model , tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Axzu9rsoGLgJ"
   },
   "outputs": [],
   "source": [
    "# í•™ìŠµëœ ëª¨ë¸ì´ ìƒì„±í•œ ìš”ì•½ë¬¸ì˜ ì¶œë ¥ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
    "def inference(config):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available()  else 'cpu')\n",
    "    print('-'*10, f'device : {device}', '-'*10,)\n",
    "    print(torch.__version__)\n",
    "\n",
    "    generate_model , tokenizer = load_tokenizer_and_model_for_test(config,device)\n",
    "\n",
    "    data_path = config['general']['data_path']\n",
    "    preprocessor = Preprocess(config['tokenizer']['bos_token'], config['tokenizer']['eos_token'])\n",
    "\n",
    "    test_data, test_encoder_inputs_dataset = prepare_test_dataset(config,preprocessor, tokenizer)\n",
    "    dataloader = DataLoader(test_encoder_inputs_dataset, batch_size=config['inference']['batch_size'])\n",
    "\n",
    "    summary = []\n",
    "    text_ids = []\n",
    "    with torch.no_grad():\n",
    "        for item in tqdm(dataloader):\n",
    "            text_ids.extend(item['ID'])\n",
    "            generated_ids = generate_model.generate(input_ids=item['input_ids'].to('cuda:0'),\n",
    "                            no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],\n",
    "                            early_stopping=config['inference']['early_stopping'],\n",
    "                            max_length=config['inference']['generate_max_length'],\n",
    "                            num_beams=config['inference']['num_beams'],\n",
    "                        )\n",
    "            for ids in generated_ids:\n",
    "                result = tokenizer.decode(ids)\n",
    "                summary.append(result)\n",
    "\n",
    "    # ì •í™•í•œ í‰ê°€ë¥¼ ìœ„í•˜ì—¬ ë…¸ì´ì¦ˆì— í•´ë‹¹ë˜ëŠ” ìŠ¤í˜ì…œ í† í°ì„ ì œê±°í•©ë‹ˆë‹¤.\n",
    "    remove_tokens = config['inference']['remove_tokens']\n",
    "    preprocessed_summary = summary.copy()\n",
    "    for token in remove_tokens:\n",
    "        preprocessed_summary = [sentence.replace(token,\" \") for sentence in preprocessed_summary]\n",
    "        \n",
    "    # âœ… ê³µë°±/íŠ¹ìˆ˜ë¬¸ì ì¶”ê°€ ì •ë¦¬\n",
    "    cleaned_summary = []\n",
    "    for s in preprocessed_summary:\n",
    "        s = s.strip()                         # ì•ë’¤ ê³µë°± ì œê±°\n",
    "        s = re.sub(r'\\s+', ' ', s)            # ì—¬ëŸ¬ ê³µë°± â†’ í•˜ë‚˜\n",
    "        s = s.strip('\"').strip(\"'\")           # ë”°ì˜´í‘œ ì œê±°\n",
    "        cleaned_summary.append(s)    \n",
    "\n",
    "    output = pd.DataFrame(\n",
    "        {\n",
    "            \"fname\": test_data['fname'],\n",
    "            \"summary\" : cleaned_summary,\n",
    "        }\n",
    "    )\n",
    "    result_path = config['inference']['result_path']\n",
    "    if not os.path.exists(result_path):\n",
    "        os.makedirs(result_path)\n",
    "    output.to_csv(os.path.join(result_path, \"output.csv\"), index=False)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "-pJ1ZXf-5V50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- device : cuda:0 ----------\n",
      "2.8.0+cu128\n",
      "---------- Load tokenizer & model ----------\n",
      "---------- Model Name : digit82/kobart-summarization ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Load tokenizer & model complete ----------\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "test_data:\n",
      "#Person1#: Ms. Dawson, ë°›ì•„ì“°ê¸° ì¢€ ë¶€íƒë“œë ¤ì•¼ê² ì–´ìš”. #Person2#: ë„¤, ë§ì”€í•˜ì„¸ìš”... #Person1#: ì´ê±¸ ì˜¤ëŠ˜ ì˜¤í›„ê¹Œì§€ ëª¨ë“  ì§ì›ë“¤ì—ê²Œ ì‚¬ë‚´ ë©”ëª¨ë¡œ ë³´ë‚´ì•¼ í•´ìš”. ì¤€ë¹„ëë‚˜ìš”? #Person2#: ë„¤, ë§ì”€í•˜ì„¸ìš”. #Person1#: ëª¨ë“  ì§ì›ì—ê²Œ ì•Œë¦½ë‹ˆë‹¤... ì¦‰ì‹œ ë°œíš¨ë˜ì–´ ëª¨ë“  ì‚¬ë‚´ í†µì‹ ì€ ì´ë©”ì¼ê³¼ ê³µì‹ ë©”ëª¨ë¡œë§Œ ì œí•œë©ë‹ˆë‹¤. ê·¼ë¬´ ì‹œê°„ ë™ì•ˆ ì¦‰ì‹œ ë©”ì‹œì§€ í”„ë¡œê·¸ë¨ ì‚¬ìš©ì€ ê¸ˆì§€ë©ë‹ˆë‹¤. #Person2#: ì´ ì •ì±…ì´ ì‚¬ë‚´ í†µì‹ ì—ë§Œ ì ìš©ë˜ë‚˜ìš”, ì•„ë‹ˆë©´ ì™¸ë¶€ í†µì‹ ì—ë„ í•´ë‹¹ë˜ë‚˜ìš”? #Person1#: ì´ëŠ” ëª¨ë“  í†µì‹ ì— ì ìš©ë©ë‹ˆë‹¤. ì‚¬ë¬´ì‹¤ ë‚´ ì§ì› ê°„ í†µì‹  ë¿ë§Œ ì•„ë‹ˆë¼ ì™¸ë¶€ í†µì‹ ë„ í•´ë‹¹ë©ë‹ˆë‹¤. #Person2#: í•˜ì§€ë§Œ ë§ì€ ì§ì›ë“¤ì´ ê³ ê°ê³¼ ì†Œí†µí•˜ë ¤ê³  ì¦‰ì‹œ ë©”ì‹œì§€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. #Person1#: í†µì‹  ë°©ë²•ì„ ë°”ê¿”ì•¼ í•  ê²ƒì…ë‹ˆë‹¤. ì´ ì‚¬ë¬´ì‹¤ì—ì„œëŠ” ì¦‰ì‹œ ë©”ì‹œì§€ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë„ˆë¬´ ë§ì€ ì‹œê°„ì´ ë‚­ë¹„ë©ë‹ˆë‹¤! ì´ì œ ê³„ì†í•´ì„œ ë©”ëª¨ë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”. ì–´ë””ê¹Œì§€ í–ˆì£ ? #Person2#: ë‚´ì™¸ë¶€ í†µì‹ ì— ì ìš©ë©ë‹ˆë‹¤. #Person1#: ë„¤. ì¦‰ì‹œ ë©”ì‹œì§€ë¥¼ ê³„ì† ì‚¬ìš©í•˜ë©´ ê²½ê³  í›„ ì‹œì • ì¡°ì¹˜ê°€ ì´ë£¨ì–´ì§€ë©°, ë‘ ë²ˆì§¸ ìœ„ë°˜ ì‹œ í•´ê³ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë²ˆ ì •ì±…ì— ê´€í•œ ì§ˆë¬¸ì€ ë¶€ì„œì¥ì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”. #Person2#: ê·¸ê²Œ ë‹¤ì¸ê°€ìš”? #Person1#: ë„¤. ì˜¤ëŠ˜ ì˜¤í›„ 4ì‹œê¹Œì§€ ì´ ë©”ëª¨ë¥¼ ì‘ì„±í•˜ê³  ë°°í¬í•´ì£¼ì„¸ìš”.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "---------- Load data complete ----------\n",
      "---------- Make dataset complete ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:59<00:00,  3.71s/it]\n"
     ]
    }
   ],
   "source": [
    "# í•™ìŠµëœ ëª¨ë¸ì˜ testë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.\n",
    "if __name__ == \"__main__\":\n",
    "    output = inference(loaded_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "OsPmLfhbzZqS"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>Ms. Dawsonì€ #Person1# ì—ê²Œ ì‚¬ë‚´ ë©”ëª¨ë¥¼ ì‘ì„±í•˜ê³  ë°°í¬í•˜ë¼ê³  ìš”ì²­í•©...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>#Person1# ê³¼ #Person2# ëŠ” êµí†µì²´ì¦ìœ¼ë¡œ ì¸í•´ ì¶œí‡´ê·¼ê¸¸ì— ëŒ€í•´ ë…¼ì˜í•˜...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>KateëŠ” #Person1# ì—ê²Œ Mashaì™€ Heroê°€ ë‘ ë‹¬ ë™ì•ˆ ë³„ê±° ì¤‘ì´ë©° ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>#Person1# ì€ Brianì˜ ìƒì¼ì„ ì¶•í•˜í•˜ë©° íŒŒí‹°ì— ì´ˆëŒ€í•©ë‹ˆë‹¤. #Person...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>#Person1# ê³¼ #Person2# ëŠ” ì˜¬ë¦¼í”½ ê³µì›ì˜ í¬ê¸°ì™€ ì‹œì„¤ì— ëŒ€í•´ ì´ì•¼ê¸°...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>test_495</td>\n",
       "      <td>Jackì€ Charlieì—ê²Œ ê³µí•­ì—ì„œ í”½ì—…í•´ì•¼ í•´ì„œ ìƒˆë¡œìš´ ë¹„ë””ì˜¤ ê²Œì„ì„ í•˜ìê³  ì´ˆ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>test_496</td>\n",
       "      <td>#Person2# ëŠ” ì‹œê³¨ ìŒì•…ì— ê´€ì‹¬ì„ ê°€ì§€ê²Œ ëœ ê³„ê¸°ì™€ ë¼ë””ì˜¤ ë°©ì†¡êµ­ì—ì„œ ì¼í•˜ê²Œ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>test_497</td>\n",
       "      <td>#Person1# ì€ Aliceì—ê²Œ ì„¸íƒê¸°ì— ë¹„ëˆ„ê°€ ë“¤ì–´ ìˆì§€ ì•Šë‹¤ëŠ” ê²ƒì„ ë°œê²¬í•˜ê³ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>test_498</td>\n",
       "      <td>MatthewëŠ” Steveì—ê²Œ ì„ëŒ€ë¥¼ ê°±ì‹ í•˜ê³  ì‹¶ì§€ ì•Šì•„ ì§‘ì„ ì°¾ê³  ìˆë‹¤ê³  ë§í•œë‹¤....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>test_499</td>\n",
       "      <td>BetsyëŠ” ìŠ¹ì§„í•˜ì—¬ ì¹œêµ¬ë“¤ì„ ì´ˆëŒ€í•´ í° íŒŒí‹°ë¥¼ ì—´ ê³„íšì´ë‹¤. Frankë„ íŒŒí‹°ì— ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>499 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        fname                                            summary\n",
       "0      test_0  Ms. Dawsonì€ #Person1# ì—ê²Œ ì‚¬ë‚´ ë©”ëª¨ë¥¼ ì‘ì„±í•˜ê³  ë°°í¬í•˜ë¼ê³  ìš”ì²­í•©...\n",
       "1      test_1  #Person1# ê³¼ #Person2# ëŠ” êµí†µì²´ì¦ìœ¼ë¡œ ì¸í•´ ì¶œí‡´ê·¼ê¸¸ì— ëŒ€í•´ ë…¼ì˜í•˜...\n",
       "2      test_2  KateëŠ” #Person1# ì—ê²Œ Mashaì™€ Heroê°€ ë‘ ë‹¬ ë™ì•ˆ ë³„ê±° ì¤‘ì´ë©° ...\n",
       "3      test_3  #Person1# ì€ Brianì˜ ìƒì¼ì„ ì¶•í•˜í•˜ë©° íŒŒí‹°ì— ì´ˆëŒ€í•©ë‹ˆë‹¤. #Person...\n",
       "4      test_4  #Person1# ê³¼ #Person2# ëŠ” ì˜¬ë¦¼í”½ ê³µì›ì˜ í¬ê¸°ì™€ ì‹œì„¤ì— ëŒ€í•´ ì´ì•¼ê¸°...\n",
       "..        ...                                                ...\n",
       "494  test_495  Jackì€ Charlieì—ê²Œ ê³µí•­ì—ì„œ í”½ì—…í•´ì•¼ í•´ì„œ ìƒˆë¡œìš´ ë¹„ë””ì˜¤ ê²Œì„ì„ í•˜ìê³  ì´ˆ...\n",
       "495  test_496  #Person2# ëŠ” ì‹œê³¨ ìŒì•…ì— ê´€ì‹¬ì„ ê°€ì§€ê²Œ ëœ ê³„ê¸°ì™€ ë¼ë””ì˜¤ ë°©ì†¡êµ­ì—ì„œ ì¼í•˜ê²Œ...\n",
       "496  test_497  #Person1# ì€ Aliceì—ê²Œ ì„¸íƒê¸°ì— ë¹„ëˆ„ê°€ ë“¤ì–´ ìˆì§€ ì•Šë‹¤ëŠ” ê²ƒì„ ë°œê²¬í•˜ê³ ...\n",
       "497  test_498  MatthewëŠ” Steveì—ê²Œ ì„ëŒ€ë¥¼ ê°±ì‹ í•˜ê³  ì‹¶ì§€ ì•Šì•„ ì§‘ì„ ì°¾ê³  ìˆë‹¤ê³  ë§í•œë‹¤....\n",
       "498  test_499  BetsyëŠ” ìŠ¹ì§„í•˜ì—¬ ì¹œêµ¬ë“¤ì„ ì´ˆëŒ€í•´ í° íŒŒí‹°ë¥¼ ì—´ ê³„íšì´ë‹¤. Frankë„ íŒŒí‹°ì— ...\n",
       "\n",
       "[499 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output  # ê° ëŒ€í™”ë¬¸ì— ëŒ€í•œ ìš”ì•½ë¬¸ì´ ì¶œë ¥ë¨ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "083ea69907bb48d4a8fff919bac51aad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "08d05bc20a96432badd459e1ffaf868e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "13651c09564a4337b8274c1cb436faa5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "14f6c91d6c634379b498586c51e606e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "21d2e54b5a0a4f79973a512105da43eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2307c6dcbe0141acb5e61baae19cade7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "285007b45236478ca147c6df752c8da4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2a190bda0b72407e9a953cd2104dd3b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2fd3d7bbcd6948d8904d33001f95ea03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3645438ace1f4596a8dbc157b48c1521": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_14f6c91d6c634379b498586c51e606e0",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_08d05bc20a96432badd459e1ffaf868e",
      "value": " 295/295 [00:00&lt;00:00, 21.3kB/s]"
     }
    },
    "3a04e871b74b45d7bf02fd33bb103577": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3bcd6b6b956347b29e1efa20a1d00542": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c284a826f6843f6aa47eacad478ac30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_affff1d8a89e4c14955d1b2aa39ff1ab",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_13651c09564a4337b8274c1cb436faa5",
      "value": "tokenizer.json: 100%"
     }
    },
    "45187decb58b4ad39ad532259c6277e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4747b668e2fa4ab58a449446f80030f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "52095cc7087243ac916055e569fd22f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c18f0e3bc35e44d9915c3f84cd282a26",
      "max": 109,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3a04e871b74b45d7bf02fd33bb103577",
      "value": 109
     }
    },
    "58001a60eacc44d5b38a68648adccde4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "58c794fb7ce543a39fdf66d757f6eeab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6f5fde5b0ac840a18bd5cc380e564ff6",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_45187decb58b4ad39ad532259c6277e5",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "5dfcf310ca9e4e2794076098a5d69cea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3c284a826f6843f6aa47eacad478ac30",
       "IPY_MODEL_6caedd60c6b747469c82930be1f95d6d",
       "IPY_MODEL_64f2218f899d446393cfea44f206f0a6"
      ],
      "layout": "IPY_MODEL_d068f541df3f438dbd5138863e64b2f2"
     }
    },
    "64f2218f899d446393cfea44f206f0a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d22fbc2c5dbf422399e496c9b500025a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_775d8bbeceac4e2da4f21ab6235c89ed",
      "value": " 682k/682k [00:00&lt;00:00, 5.40MB/s]"
     }
    },
    "6caedd60c6b747469c82930be1f95d6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3bcd6b6b956347b29e1efa20a1d00542",
      "max": 682133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2fd3d7bbcd6948d8904d33001f95ea03",
      "value": 682133
     }
    },
    "6f5fde5b0ac840a18bd5cc380e564ff6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "775d8bbeceac4e2da4f21ab6235c89ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8a6464a355f7464c989033965d418a8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2307c6dcbe0141acb5e61baae19cade7",
      "max": 295,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4747b668e2fa4ab58a449446f80030f5",
      "value": 295
     }
    },
    "a15af9e8158f4903b9189f3d322a5ef3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ac00d6c2cf974b33a628acb3f1471316",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_285007b45236478ca147c6df752c8da4",
      "value": " 109/109 [00:00&lt;00:00, 9.44kB/s]"
     }
    },
    "ac00d6c2cf974b33a628acb3f1471316": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "affff1d8a89e4c14955d1b2aa39ff1ab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c18f0e3bc35e44d9915c3f84cd282a26": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d068f541df3f438dbd5138863e64b2f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d22fbc2c5dbf422399e496c9b500025a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de1a3f7701c243839fe03b930a9b9e30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ebc22683058a4f229c5588e52fc93536",
       "IPY_MODEL_52095cc7087243ac916055e569fd22f3",
       "IPY_MODEL_a15af9e8158f4903b9189f3d322a5ef3"
      ],
      "layout": "IPY_MODEL_21d2e54b5a0a4f79973a512105da43eb"
     }
    },
    "e920dbc173c045d1a32143349f1dff8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_58c794fb7ce543a39fdf66d757f6eeab",
       "IPY_MODEL_8a6464a355f7464c989033965d418a8a",
       "IPY_MODEL_3645438ace1f4596a8dbc157b48c1521"
      ],
      "layout": "IPY_MODEL_58001a60eacc44d5b38a68648adccde4"
     }
    },
    "ebc22683058a4f229c5588e52fc93536": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_083ea69907bb48d4a8fff919bac51aad",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2a190bda0b72407e9a953cd2104dd3b2",
      "value": "special_tokens_map.json: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
