import os
import yaml
from glob import glob
from pprint import pprint
from typing import List, Dict, Tuple, Any, Optional

import pandas as pd
import torch
from tqdm import tqdm
from rouge import Rouge
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer,
    EarlyStoppingCallback
)
from transformers.modeling_utils import PreTrainedModel
from rouge_score import rouge_scorer


# --------------------------------------------------------------------------------------
# ì „ì—­ ì„¤ì • ë° PEP 8 ì¤€ìˆ˜ (ìƒìˆ˜ëŠ” ëŒ€ë¬¸ì ìŠ¤ë„¤ì´í¬ ì¼€ì´ìŠ¤)
# --------------------------------------------------------------------------------------
CONFIG_PATH: str = "./config.yaml"


# --------------------------------------------------------------------------------------
# config.yaml íŒŒì¼ì—ì„œ ì„¤ì •ê°’ì„ ì§ì ‘ ë¶ˆëŸ¬ì˜¤ë„ë¡ ë³€ê²½
# --------------------------------------------------------------------------------------
try:
    with open(CONFIG_PATH, "r", encoding="utf-8") as file:
        loaded_config: Dict[str, Any] = yaml.safe_load(file)
except FileNotFoundError:
    print(f"ì˜¤ë¥˜: ì„¤ì • íŒŒì¼ '{CONFIG_PATH}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. config.yamlì„ ë¨¼ì € ìƒì„±í•˜ì„¸ìš”.")
    # PEP 20: Errors should never pass silently.
    exit(1) # ì—ëŸ¬ ì‹œ exit(1) ì‚¬ìš©

# ë¶ˆëŸ¬ì˜¨ config ë‚´ìš©ì„ ì¶œë ¥
pprint(loaded_config)
print(loaded_config.get('general'))
print(loaded_config.get('tokenizer'))
print(loaded_config.get('training'))
print(loaded_config.get('wandb'))
print(loaded_config.get('inference'))

data_path: str = loaded_config['general']['data_path']

# train dataì™€ validation data ë¶ˆëŸ¬ì˜¤ê¸°
try:
    # ë³€ìˆ˜ëª…ì„ PEP 8ì— ë”°ë¼ ìŠ¤ë„¤ì´í¬ ì¼€ì´ìŠ¤ë¡œ ìœ ì§€
    train_df: pd.DataFrame = pd.read_csv(os.path.join(data_path, 'train.csv'))
    print(train_df.tail())
    val_df: pd.DataFrame = pd.read_csv(os.path.join(data_path, 'dev.csv'))
    print(val_df.tail())
except FileNotFoundError as e:
    print(f"ë°ì´í„° íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}. 'data_path' ì„¤ì •ì„ í™•ì¸í•´.")
    # í•™ìŠµ/ì¶”ë¡ ì´ ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ ì—ëŸ¬ ë°œìƒ ì‹œ ì¢…ë£Œ
    # exit(1)


class Preprocess:
    """
    ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ í´ë˜ìŠ¤. ë°ì´í„°ì…‹ì„ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜í•˜ê³ 
    ì¸ì½”ë”(dialogue)ì™€ ë””ì½”ë”(summary)ì˜ ì…ë ¥ì„ ìƒì„±.
    """
    def __init__(
        self,
        bos_token: str,
        eos_token: str,
    ) -> None:
        self.bos_token: str = bos_token
        self.eos_token: str = eos_token

    @staticmethod
    def make_set_as_df(file_path: str, is_train: bool = True) -> pd.DataFrame:
        """ì‹¤í—˜ì— í•„ìš”í•œ ì»¬ëŸ¼(fname, dialogue, summary)ì„ ê°€ì§„ ë°ì´í„°í”„ë ˆì„ì„ ìƒì„±."""
        df: pd.DataFrame = pd.read_csv(file_path)
        if is_train:
            return df[['fname', 'dialogue', 'summary', 'topic']]

        return df[['fname', 'dialogue']]

    def make_input(
        self,
        dataset: pd.DataFrame,
        is_test: bool = False
    ) -> Tuple[List[str], List[str], Optional[List[str]]]:
        """BART/T5 ëª¨ë¸ì˜ ì…ë ¥ í˜•íƒœë¥¼ ë§ì¶”ê¸° ìœ„í•´ ì „ì²˜ë¦¬ë¥¼ ì§„í–‰."""
        # \n ë° <br> ê°™ì€ ì¤„ ë°”ê¿ˆ ë…¸ì´ì¦ˆë¥¼ ê³µë°±ìœ¼ë¡œ í†µì¼
        cleaned_dialogues: pd.Series = dataset['dialogue'].apply(
            lambda x: str(x).replace('\n', ' ').replace('<br>', ' ')
        )

        if is_test:
            encoder_input: List[str] = cleaned_dialogues.tolist()
            # í…ŒìŠ¤íŠ¸ ì‹œì—ëŠ” ì‹¤ì œ ìš”ì•½ ëŒ€ì‹  BOS í† í°ë§Œ ë””ì½”ë” ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
            decoder_input: List[str] = [self.bos_token] * len(cleaned_dialogues)
            return encoder_input, decoder_input, None # ë°˜í™˜ íƒ€ì…ì„ ë§ì¶”ê¸° ìœ„í•´ None ì¶”ê°€
        else:
            encoder_input: List[str] = cleaned_dialogues.tolist()
            # Ground truthë¥¼ ë””ì½”ë”ì˜ inputìœ¼ë¡œ ì‚¬ìš© (BOS í† í° ì¶”ê°€)
            decoder_input: List[str] = dataset['summary'].apply(
                lambda x: self.bos_token + str(x)
            ).tolist()
            # Ground truthë¥¼ ë ˆì´ë¸”ë¡œ ì‚¬ìš© (EOS í† í° ì¶”ê°€)
            decoder_output: List[str] = dataset['summary'].apply(
                lambda x: str(x) + self.eos_token
            ).tolist()
            return encoder_input, decoder_input, decoder_output


class DatasetForTrain(Dataset):
    """ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©ë˜ëŠ” Dataset í´ë˜ìŠ¤."""
    def __init__(
        self,
        encoder_input: Dict[str, torch.Tensor],
        decoder_input: Dict[str, torch.Tensor],
        labels: Dict[str, torch.Tensor],
        length: int
    ) -> None:
        self.encoder_input: Dict[str, torch.Tensor] = encoder_input
        self.decoder_input: Dict[str, torch.Tensor] = decoder_input
        self.labels: Dict[str, torch.Tensor] = labels
        self._length: int = length

    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        # ì¸ì½”ë” ì…ë ¥ (dialogue)
        item: Dict[str, torch.Tensor] = {
            key: val[idx].clone().detach() for key, val in self.encoder_input.items()
        }
        # ë””ì½”ë” ì…ë ¥ (summary with BOS)
        item_decoder: Dict[str, torch.Tensor] = {
            key: val[idx].clone().detach() for key, val in self.decoder_input.items()
        }

        # Seq2Seq ëª¨ë¸ ì…ë ¥ í‚¤ ì´ë¦„ ë³€ê²½
        item_decoder['decoder_input_ids'] = item_decoder.pop('input_ids')
        item_decoder['decoder_attention_mask'] = item_decoder.pop('attention_mask')
        # 'attention_ids' í‚¤ëŠ” ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì œê±° ì‹œ ì˜¤ë¥˜ ë°œìƒ ë°©ì§€
        item_decoder.pop('attention_ids', None)
        
        item.update(item_decoder)
        # ë ˆì´ë¸” (summary with EOS)
        item['labels'] = self.labels['input_ids'][idx].clone().detach()
        return item

    def __len__(self) -> int:
        return self._length


# Validationì— ì‚¬ìš©ë˜ëŠ” Dataset í´ë˜ìŠ¤ëŠ” Trainê³¼ ë™ì¼í•œ êµ¬ì¡°ë¥¼ ê°€ì§‘ë‹ˆë‹¤.
class DatasetForVal(DatasetForTrain):
    """ëª¨ë¸ ê²€ì¦ì— ì‚¬ìš©ë˜ëŠ” Dataset í´ë˜ìŠ¤."""
    pass


class DatasetForInference(Dataset):
    """ëª¨ë¸ ì¶”ë¡ ì— ì‚¬ìš©ë˜ëŠ” Dataset í´ë˜ìŠ¤."""
    def __init__(self, encoder_input: Dict[str, torch.Tensor], test_id: pd.Series, length: int) -> None:
        self.encoder_input: Dict[str, torch.Tensor] = encoder_input
        self.test_id: pd.Series = test_id
        self._length: int = length

    def __getitem__(self, idx: int) -> Dict[str, Any]:
        item: Dict[str, torch.Tensor] = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}
        # IDëŠ” ë¬¸ìì—´ì´ë¯€ë¡œ í…ì„œê°€ ì•„ë‹Œ ê·¸ëŒ€ë¡œ ë°˜í™˜
        item['ID'] = self.test_id.iloc[idx]
        return item

    def __len__(self) -> int:
        return self._length


def prepare_train_dataset(
    config: Dict[str, Any],
    preprocessor: Preprocess,
    data_path: str,
    tokenizer: AutoTokenizer
) -> Tuple[DatasetForTrain, DatasetForVal]:
    """í›ˆë ¨ ë° ê²€ì¦ ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ê³  í† í°í™”í•˜ì—¬ Dataset ê°ì²´ë¡œ ë°˜í™˜."""
    train_file_path: str = os.path.join(data_path, 'train.csv')
    val_file_path: str = os.path.join(data_path, 'dev.csv')

    train_data: pd.DataFrame = preprocessor.make_set_as_df(train_file_path)
    val_data: pd.DataFrame = preprocessor.make_set_as_df(val_file_path)

    print('-' * 150)
    print(f'train_data:\n{train_data["dialogue"].iloc[0]}')
    print(f'train_label:\n{train_data["summary"].iloc[0]}')

    print('-' * 150)
    print(f'val_data:\n{val_data["dialogue"].iloc[0]}')
    print(f'val_label:\n{val_data["summary"].iloc[0]}')

    # PEP 8 ì¤€ìˆ˜ë¥¼ ìœ„í•´ ë³€ìˆ˜ëª…ì— _train, _val, _ouputs ëŒ€ì‹  ìŠ¤ë„¤ì´í¬ ì¼€ì´ìŠ¤ ì‚¬ìš©
    encoder_input_train, decoder_input_train, decoder_output_train = preprocessor.make_input(train_data)
    encoder_input_val, decoder_input_val, decoder_output_val = preprocessor.make_input(val_data)
    print('---------- Load data complete ----------')

    tokenizer_config: Dict[str, Any] = config['tokenizer']

    # í† í¬ë‚˜ì´ì§• (í›ˆë ¨ ë°ì´í„°)
    tokenized_encoder_inputs: Dict[str, torch.Tensor] = tokenizer(
        encoder_input_train, return_tensors="pt", padding=True,
        add_special_tokens=True, truncation=True,
        max_length=tokenizer_config['encoder_max_len'],
        return_token_type_ids=False
    )
    tokenized_decoder_inputs: Dict[str, torch.Tensor] = tokenizer(
        decoder_input_train, return_tensors="pt", padding=True,
        add_special_tokens=True, truncation=True,
        max_length=tokenizer_config['decoder_max_len'],
        return_token_type_ids=False
    )
    tokenized_decoder_outputs: Dict[str, torch.Tensor] = tokenizer(
        decoder_output_train, return_tensors="pt", padding=True,
        add_special_tokens=True, truncation=True,
        max_length=tokenizer_config['decoder_max_len'],
        return_token_type_ids=False
    )

    train_inputs_dataset: DatasetForTrain = DatasetForTrain(
        tokenized_encoder_inputs, tokenized_decoder_inputs,
        tokenized_decoder_outputs, len(encoder_input_train)
    )

    # í† í¬ë‚˜ì´ì§• (ê²€ì¦ ë°ì´í„°)
    val_tokenized_encoder_inputs: Dict[str, torch.Tensor] = tokenizer(
        encoder_input_val, return_tensors="pt", padding=True,
        add_special_tokens=True, truncation=True,
        max_length=tokenizer_config['encoder_max_len'],
        return_token_type_ids=False
    )
    val_tokenized_decoder_inputs: Dict[str, torch.Tensor] = tokenizer(
        decoder_input_val, return_tensors="pt", padding=True,
        add_special_tokens=True, truncation=True,
        max_length=tokenizer_config['decoder_max_len'],
        return_token_type_ids=False
    )
    val_tokenized_decoder_outputs: Dict[str, torch.Tensor] = tokenizer(
        decoder_output_val, return_tensors="pt", padding=True,
        add_special_tokens=True, truncation=True,
        max_length=tokenizer_config['decoder_max_len'],
        return_token_type_ids=False
    )

    val_inputs_dataset: DatasetForVal = DatasetForVal(
        val_tokenized_encoder_inputs, val_tokenized_decoder_inputs,
        val_tokenized_decoder_outputs, len(encoder_input_val)
    )

    print('---------- Make dataset complete ----------')
    return train_inputs_dataset, val_inputs_dataset


def compute_metrics(config: Dict[str, Any], tokenizer: AutoTokenizer, pred: Any) -> Dict[str, float]:
    """
    ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼(pred)ë¥¼ ë°›ì•„ ROUGE ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ì—¬ ë°˜í™˜.
    """
    try:
        # 'rouge' ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ Rouge í´ë˜ìŠ¤ ì‚¬ìš©
        rouge_metric: Rouge = Rouge()  

        predictions: Any = pred.predictions
        labels: Any = pred.label_ids

        # -100 (ignore_index)ì„ ì‹¤ì œ íŒ¨ë”© í† í° IDë¡œ ëŒ€ì²´
        predictions[predictions == -100] = tokenizer.pad_token_id
        labels[labels == -100] = tokenizer.pad_token_id

        decoded_preds: List[str] = tokenizer.batch_decode(predictions, clean_up_tokenization_spaces=True)
        decoded_labels: List[str] = tokenizer.batch_decode(labels, clean_up_tokenization_spaces=True)

        # ë¶ˆí•„ìš”í•œ ìƒì„±í† í° ì œê±°
        replaced_predictions: List[str] = decoded_preds.copy()
        replaced_labels: List[str] = decoded_labels.copy()
        remove_tokens: List[str] = config['inference']['remove_tokens']

        for token in remove_tokens:
            replaced_predictions = [sentence.replace(token, " ").strip() for sentence in replaced_predictions]
            replaced_labels = [sentence.replace(token, " ").strip() for sentence in replaced_labels]

        # ë¡œê·¸ ì¶œë ¥ (ì²« 3ê°œ ìƒ˜í”Œ)
        print('-' * 150)
        print(f"PRED: {replaced_predictions[0]}")
        print(f"GOLD: {replaced_labels[0]}")
        print('-' * 150)
        print(f"PRED: {replaced_predictions[1]}")
        print(f"GOLD: {replaced_labels[1]}")
        print('-' * 150)
        print(f"PRED: {replaced_predictions[2]}")
        print(f"GOLD: {replaced_labels[2]}")
        print('-' * 150)

        # ìµœì¢…ì ì¸ ROUGE ì ìˆ˜ë¥¼ ê³„ì‚°
        # ROUGE ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ë¹ˆ ë¬¸ìì—´ì—ì„œ ì˜¤ë¥˜ê°€ ë‚  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¹ˆ ë¬¸ìì—´ì€ ì œê±°
        valid_preds: List[str] = [p for p in replaced_predictions if p.strip()]
        # ì˜ˆì¸¡ ë¬¸ìì—´ì´ ìœ íš¨í•œ ê²½ìš°ì—ë§Œ í•´ë‹¹ ë ˆì´ë¸”ì„ ì‚¬ìš©
        valid_labels: List[str] = [l for p, l in zip(replaced_predictions, replaced_labels) if p.strip()]

        if not valid_preds:
            print("ê²½ê³ : ìœ íš¨í•œ ì˜ˆì¸¡ ë¬¸ìì—´ì´ ì—†ìŠµë‹ˆë‹¤.")
            return {}

        # get_scoresì˜ ì…ë ¥ìœ¼ë¡œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ê°€ ë“¤ì–´ê°€ì§€ ì•Šë„ë¡ ìœ íš¨ì„± ê²€ì‚¬
        if not valid_preds or not valid_labels:
            print("ê²½ê³ : ìœ íš¨í•œ ì˜ˆì¸¡ ë˜ëŠ” ë ˆì´ë¸”ì´ ì—†ì–´ ROUGE ì ìˆ˜ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {}

        # 'rouge' ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ get_scoresë¥¼ ì‚¬ìš©
        results: List[Dict[str, Dict[str, float]]] = rouge_metric.get_scores(
            valid_preds, valid_labels, avg=True
        )

        # ROUGE ì ìˆ˜ ì¤‘ F-1 scoreë¥¼ í†µí•´ í‰ê°€
        result: Dict[str, float] = {}
        for key, value in results.items():
            result[key] = value["f"]

        # ì†Œìˆ˜ì  4ìë¦¬ê¹Œì§€ ë°˜ì˜¬ë¦¼
        result = {k: round(v * 100, 4) for k, v in result.items()}

        return result

    except Exception as e:
        print(f"Error computing metrics: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•˜ì—¬ Trainerê°€ ë©ˆì¶”ì§€ ì•Šë„ë¡ í•¨
        return {}


def load_trainer_for_train(
    config: Dict[str, Any],
    model: PreTrainedModel,
    tokenizer: AutoTokenizer,
    train_inputs_dataset: DatasetForTrain,
    val_inputs_dataset: DatasetForVal
) -> Seq2SeqTrainer:
    """Seq2SeqTrainer ê°ì²´ë¥¼ ì´ˆê¸°í™”í•˜ê³  ë°˜í™˜."""
    print('---------- Make training arguments ----------')

    # configì˜ ê°’ì„ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ Seq2SeqTrainingArguments ìƒì„±
    training_args: Seq2SeqTrainingArguments = Seq2SeqTrainingArguments(
        output_dir=config['general']['output_dir'],
        overwrite_output_dir=config['training']['overwrite_output_dir'],
        num_train_epochs=config['training']['num_train_epochs'],
        learning_rate=config['training']['learning_rate'],
        per_device_train_batch_size=config['training']['per_device_train_batch_size'],
        per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],
        warmup_ratio=config['training']['warmup_ratio'],
        weight_decay=config['training']['weight_decay'],
        lr_scheduler_type=config['training']['lr_scheduler_type'],
        optim=config['training']['optim'],
        gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],
        evaluation_strategy=config['training']['evaluation_strategy'],
        save_strategy=config['training']['save_strategy'],
        save_total_limit=config['training']['save_total_limit'],
        fp16=config['training']['fp16'],
        load_best_model_at_end=config['training']['load_best_model_at_end'],
        seed=config['training']['seed'],
        logging_dir=config['training']['logging_dir'],
        logging_strategy=config['training']['logging_strategy'],
        predict_with_generate=config['training']['predict_with_generate'],
        generation_max_length=config['training']['generation_max_length'],
        do_train=config['training']['do_train'],
        do_eval=config['training']['do_eval'],
        report_to=config['training']['report_to'],
        # ë©”ëª¨ë¦¬ ì ˆì•½ ê¸°ëŠ¥ í™œì„±í™”
        gradient_checkpointing=config['training'].get('gradient_checkpointing', False),
    )

    # Validation lossê°€ ë” ì´ìƒ ê°œì„ ë˜ì§€ ì•Šì„ ë•Œ í•™ìŠµì„ ì¤‘ë‹¨ì‹œí‚¤ëŠ” EarlyStopping ê¸°ëŠ¥
    my_callback: EarlyStoppingCallback = EarlyStoppingCallback(
        early_stopping_patience=config['training']['early_stopping_patience'],
        early_stopping_threshold=config['training']['early_stopping_threshold']
    )

    print('---------- Make training arguments complete ----------')
    print('---------- Make trainer ----------')

    # Trainer í´ë˜ìŠ¤ ì •ì˜
    trainer: Seq2SeqTrainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_inputs_dataset,
        eval_dataset=val_inputs_dataset,
        # compute_metrics í•¨ìˆ˜ì— configì™€ tokenizerë¥¼ ì¸ìë¡œ ë„˜ê¸°ë„ë¡ ìˆ˜ì •
        compute_metrics=lambda pred: compute_metrics(config, tokenizer, pred),
        callbacks=[my_callback]
    )
    print('---------- Make trainer complete ----------')

    return trainer


def load_tokenizer_and_model_for_train(
    config: Dict[str, Any],
    device: torch.device
) -> Tuple[AutoModelForSeq2SeqLM, AutoTokenizer]:
    """í•™ìŠµì„ ìœ„í•œ tokenizerì™€ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜´. PEP 8ì— ë”°ë¼ ë³€ìˆ˜ëª… ìˆ˜ì •."""
    print('---------- Load tokenizer & model ----------')

    model_name: str = config['general']['model_name']
    print(f'---------- Model Name : {model_name} ----------')

    # configì— ì„¤ì •ëœ ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ì„ ë¡œë“œ
    tokenizer: AutoTokenizer = AutoTokenizer.from_pretrained(model_name)
    model: AutoModelForSeq2SeqLM = AutoModelForSeq2SeqLM.from_pretrained(model_name)

    special_tokens_dict: Dict[str, List[str]] = {'additional_special_tokens': config['tokenizer']['special_tokens']}
    # ì•ˆì „í•˜ê²Œ ë‹¤ì‹œ ì¶”ê°€í•˜ê³  resize_token_embeddings í˜¸ì¶œ
    tokenizer.add_special_tokens(special_tokens_dict)

    # special tokenì„ ì¶”ê°€í–ˆìœ¼ë¯€ë¡œ ì„ë² ë”© ë ˆì´ì–´ í¬ê¸° ì¬ì¡°ì •
    model.resize_token_embeddings(len(tokenizer))
    model.to(device)
    print(model.config)

    print('---------- Load tokenizer & model complete ----------')
    return model, tokenizer


def find_latest_checkpoint(output_dir: str = './') -> Optional[str]:
    """ê°€ì¥ ìµœê·¼ì— ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ í´ë” ê²½ë¡œë¥¼ ì°¾ëŠ”ë‹¤."""
    # glob ëª¨ë“ˆì„ ì‚¬ìš©í•˜ì—¬ 'checkpoint-'ë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  í´ë”ë¥¼ ì°¾ìŒ
    checkpoint_dirs: List[str] = glob(f"{output_dir}/checkpoint-*")
    
    if not checkpoint_dirs:
        return None
        
    # í´ë” ì´ë¦„ ëŒ€ì‹ , ìˆ˜ì • ì‹œê°„ì„ ê¸°ì¤€ìœ¼ë¡œ ê°€ì¥ ìµœê·¼ì˜ í´ë”ë¥¼ ì„ íƒ
    # os.path.getmtime: íŒŒì¼/í´ë”ì˜ ë§ˆì§€ë§‰ ìˆ˜ì • ì‹œê°„ì„ ì´ˆ ë‹¨ìœ„ë¡œ ë°˜í™˜
    latest_checkpoint: str = max(checkpoint_dirs, key=os.path.getmtime)
    
    return latest_checkpoint


def update_inference_path_with_latest_checkpoint(config: Dict[str, Any]) -> None:
    """í•™ìŠµ í´ë”ì—ì„œ ê°€ì¥ ìµœê·¼ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œë¥¼ ì°¾ì•„ configë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤."""
    output_dir: str = config['general']['output_dir']
    
    # load_best_model_at_endê°€ Trueì¼ ë•Œ, ê°€ì¥ ìµœì ì˜ ëª¨ë¸ì´ ë§ˆì§€ë§‰ ì²´í¬í¬ì¸íŠ¸ í´ë”ì— ì €ì¥ë¨
    latest_path: Optional[str] = find_latest_checkpoint(output_dir)

    if latest_path:
        # ì°¾ì€ ìµœì‹  ê²½ë¡œë¡œ config['inference']['ckt_path']ë¥¼ ë®ì–´ì“°ê¸°
        config['inference']['ckt_path'] = latest_path  
        print(f"\nâœ¨ [ìë™ ì„¤ì •] ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ: '{latest_path}'ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
        # ì—…ë°ì´íŠ¸ëœ ê²½ë¡œë¥¼ ì €ì¥ (ì„ íƒ ì‚¬í•­ì´ì§€ë§Œ ì•ˆì „ì„ ìœ„í•´)
        try:
            with open(CONFIG_PATH, 'w', encoding='utf-8') as f:
                yaml.dump(config, f)
        except Exception as e:
            # ì˜ˆì™¸ ì²˜ë¦¬ ì‹œ ë¡œê·¸ ì¶œë ¥
            print(f"ê²½ê³ : config.yaml íŒŒì¼ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ - {e}")
    else:
        print("\nâš ï¸ [ìë™ ì„¤ì • ì‹¤íŒ¨] ì²´í¬í¬ì¸íŠ¸ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. config.yamlì˜ ckt_pathë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    

def prepare_test_dataset(
    config: Dict[str, Any],
    preprocessor: Preprocess,
    tokenizer: AutoTokenizer
) -> Tuple[pd.DataFrame, DatasetForInference]:
    """í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ê³  í† í°í™”í•˜ì—¬ Dataset ê°ì²´ë¡œ ë°˜í™˜."""
    test_file_path: str = os.path.join(config['general']['data_path'], 'test.csv')

    test_data: pd.DataFrame = preprocessor.make_set_as_df(test_file_path, is_train=False)
    test_id: pd.Series = test_data['fname']

    print('-' * 150)
    print(f'test_data:\n{test_data["dialogue"].iloc[0]}')
    print('-' * 150)

    encoder_input_test, decoder_input_test, _ = preprocessor.make_input(test_data, is_test=True)
    print('---------- Load data complete ----------')

    tokenizer_config: Dict[str, Any] = config['tokenizer']

    # í† í¬ë‚˜ì´ì§• (ì¸ì½”ë” ì…ë ¥)
    test_tokenized_encoder_inputs: Dict[str, torch.Tensor] = tokenizer(
        encoder_input_test, return_tensors="pt", padding=True,
        add_special_tokens=True, truncation=True,
        max_length=tokenizer_config['encoder_max_len'],
        return_token_type_ids=False
    )
    # í† í¬ë‚˜ì´ì§• (ë””ì½”ë” ì…ë ¥ - T5ì—ì„œëŠ” í•„ìš” ì—†ì§€ë§Œ êµ¬ì¡° ìœ ì§€ë¥¼ ìœ„í•´ í¬í•¨)
    # ë³€ìˆ˜ëª…ì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì˜ë¯¸ë¡œ ì–¸ë”ë°”(_)ë¥¼ ì•ì— ë¶™ì„ (PEP 8)
    _test_tokenized_decoder_inputs: Dict[str, torch.Tensor] = tokenizer(
        decoder_input_test, return_tensors="pt", padding=True,
        add_special_tokens=True, truncation=True,
        max_length=tokenizer_config['decoder_max_len'],
        return_token_type_ids=False
    )

    test_encoder_inputs_dataset: DatasetForInference = DatasetForInference(
        test_tokenized_encoder_inputs, test_id, len(encoder_input_test)
    )
    print('---------- Make dataset complete ----------')

    return test_data, test_encoder_inputs_dataset


def load_tokenizer_and_model_for_test(
    config: Dict[str, Any],
    device: torch.device
) -> Tuple[AutoModelForSeq2SeqLM, AutoTokenizer]:
    """ì¶”ë¡ ì„ ìœ„í•œ tokenizerì™€ í•™ìŠµëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜´. PEP 8ì— ë”°ë¼ ë³€ìˆ˜ëª… ìˆ˜ì •."""
    print('---------- Load tokenizer & model ----------')

    model_name: str = config['general']['model_name']
    ckt_path: str = config['inference']['ckt_path']
    print(f'---------- Model Name : {model_name} ----------')
    print(f'---------- Checkpoint Path : {ckt_path} ----------')

    # configì— ì„¤ì •ëœ ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œ
    tokenizer: AutoTokenizer = AutoTokenizer.from_pretrained(model_name)
    special_tokens_dict: Dict[str, List[str]] = {'additional_special_tokens': config['tokenizer']['special_tokens']}
    tokenizer.add_special_tokens(special_tokens_dict)

    # [í•µì‹¬ ìˆ˜ì •] safe_serialization=False ì˜µì…˜ì„ ì œê±°í•¨!
    model: AutoModelForSeq2SeqLM = AutoModelForSeq2SeqLM.from_pretrained(
        ckt_path,  
        low_cpu_mem_usage=True
    )
    model.resize_token_embeddings(len(tokenizer))
    model.to(device)
    print('---------- Load tokenizer & model complete ----------')

    return model, tokenizer


def inference(config: Dict[str, Any], is_final_submission: bool = False) -> pd.DataFrame:
    """
    í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë°˜í™˜.
    is_final_submission=Trueì´ë©´ test.csvë¥¼ ì‚¬ìš©, Falseì´ë©´ dev.csvë¥¼ ì‚¬ìš©.
    """
    device: torch.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    print(f'---------- device : {device} ----------')
    print(torch.__version__)

    model: AutoModelForSeq2SeqLM
    tokenizer: AutoTokenizer
    try:
        # PEP 8 ì¤€ìˆ˜ë¥¼ ìœ„í•´ ë³€ìˆ˜ëª… ìˆ˜ì •
        model, tokenizer = load_tokenizer_and_model_for_test(config, device)
    except Exception as e:
        print(f"ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}. 'ckt_path'ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return pd.DataFrame()  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ DataFrame ë°˜í™˜

    # T5 ëª¨ë¸ì˜ BOS/EOS í† í°ì€ í† í¬ë‚˜ì´ì €ì˜ ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •
    bos_token: str = tokenizer.bos_token if tokenizer.bos_token else tokenizer.pad_token
    eos_token: str = tokenizer.eos_token if tokenizer.eos_token else "</s>"

    preprocessor: Preprocess = Preprocess(bos_token, eos_token)

    # ---------------------------------------------------------------
    # [ìˆ˜ì •] ì¶”ë¡  ë°ì´í„°ì…‹ ë¡œë“œ: is_final_submission ì—¬ë¶€ì— ë”°ë¼ dev.csv ë˜ëŠ” test.csv ë¡œë“œ
    # ---------------------------------------------------------------
    data_file_name: str = 'test.csv' if is_final_submission else 'dev.csv'
    data_file_path: str = os.path.join(config['general']['data_path'], data_file_name)
    
    print(f"\nğŸš€ ì¶”ë¡  ëª¨ë“œ: {data_file_name}ì„(ë¥¼) ë¡œë“œí•©ë‹ˆë‹¤.")
    
    try:
        # test_data ë³€ìˆ˜ëª…ì„ ìœ ì§€í•˜ë©´ì„œ í˜„ì¬ í•„ìš”í•œ ë°ì´í„°ë¥¼ ë¡œë“œí•¨
        test_data: pd.DataFrame = preprocessor.make_set_as_df(data_file_path, is_train=False)  
        test_id: pd.Series = test_data['fname']

        print('-' * 150)
        print(f'test_data (FILE: {data_file_name}):\n{test_data["dialogue"].iloc[0]}')
        print('-' * 150)

        # ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ë³€ìˆ˜ì—ëŠ” _ë¥¼ ë¶™ì—¬ PEP 8 ì¤€ìˆ˜
        encoder_input_test, decoder_input_test, _ = preprocessor.make_input(test_data, is_test=True)
        print(f'---------- Load {data_file_name} data complete ----------')

        tokenizer_config: Dict[str, Any] = config['tokenizer']

        # í† í¬ë‚˜ì´ì§• (ì¸ì½”ë” ì…ë ¥)
        test_tokenized_encoder_inputs: Dict[str, torch.Tensor] = tokenizer(
            encoder_input_test, return_tensors="pt", padding=True,
            add_special_tokens=True, truncation=True,
            max_length=tokenizer_config['encoder_max_len'],
            return_token_type_ids=False
        )
        # T5ì—ì„œëŠ” ë””ì½”ë” ì…ë ¥ì´ í•„ìš” ì—†ì§€ë§Œ êµ¬ì¡° ìœ ì§€ë¥¼ ìœ„í•´ í† í¬ë‚˜ì´ì§•ë§Œ ìˆ˜í–‰ (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
        _test_tokenized_decoder_inputs: Dict[str, torch.Tensor] = tokenizer(
            decoder_input_test, return_tensors="pt", padding=True,
            add_special_tokens=True, truncation=True,
            max_length=tokenizer_config['decoder_max_len'],
            return_token_type_ids=False
        )

        test_encoder_inputs_dataset: DatasetForInference = DatasetForInference(
            test_tokenized_encoder_inputs, test_id, len(encoder_input_test)
        )
        print(f'---------- Make {data_file_name} dataset complete ----------')
        
    except Exception as e:
        print(f"ë°ì´í„°ì…‹ ì¤€ë¹„ ì˜¤ë¥˜: {e}. 'data_path'ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return pd.DataFrame()  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ DataFrame ë°˜í™˜
    # ---------------------------------------------------------------


    dataloader: DataLoader = DataLoader(
        test_encoder_inputs_dataset,
        batch_size=config['inference']['batch_size'],
        shuffle=False
    )

    summary: List[str] = []
    text_ids: List[str] = []
    model.eval()  
    with torch.no_grad():
        for item in tqdm(dataloader):
            # IDëŠ” ë¬¸ìì—´(str)ì´ë¯€ë¡œ ë¦¬ìŠ¤íŠ¸ì— ë‹´ì„ ë•Œ extend
            text_ids.extend(item['ID'])

            # CUDA ì¥ì¹˜ë¡œ í…ì„œë¥¼ ì´ë™
            input_ids: torch.Tensor = item['input_ids'].to(device)

            generated_ids: torch.Tensor = model.generate(
                input_ids=input_ids,
                no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],
                early_stopping=config['inference']['early_stopping'],
                max_length=config['inference']['generate_max_length'],
                num_beams=config['inference']['num_beams'],
            )
            for ids in generated_ids:
                result: str = tokenizer.decode(ids)
                summary.append(result)

    # ì •í™•í•œ í‰ê°€ë¥¼ ìœ„í•˜ì—¬ ë…¸ì´ì¦ˆì— í•´ë‹¹ë˜ëŠ” ìŠ¤í˜ì…œ í† í°ì„ ì œê±°
    remove_tokens: List[str] = config['inference']['remove_tokens']
    preprocessed_summary: List[str] = summary.copy()
    for token in remove_tokens:
        # replace í›„ stripì„ ì¶”ê°€í•˜ì—¬ ê¹”ë”í•œ ê²°ê³¼ ë³´ì¥
        preprocessed_summary = [sentence.replace(token, " ").strip() for sentence in preprocessed_summary]

    output: pd.DataFrame = pd.DataFrame(
        {
            "fname": text_ids, # [í•µì‹¬ ìˆ˜ì •] test_data['fname'] ëŒ€ì‹  text_ids ì‚¬ìš© (ìˆœì„œ ë³´ì¥)
            "summary": preprocessed_summary,
        }
    )
    result_path: str = config['inference']['result_path']
    if not os.path.exists(result_path):
        os.makedirs(result_path)
    
    # ìµœì¢… ì œì¶œ íŒŒì¼ì€ prediction.csvë¡œ ì €ì¥í•˜ë„ë¡ ìˆ˜ì •
    output_filename: str = "prediction.csv" if is_final_submission else "output.csv"
    output.to_csv(os.path.join(result_path, output_filename), index=False)  

    return output


def main(config: Dict[str, Any]) -> None:
    """
    í•™ìŠµ ë˜ëŠ” ì¶”ë¡ ì„ ì‹œì‘í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜.
    config['training']['do_train'] ê°’ì— ë”°ë¼ ì‘ë™ì´ ë‹¬ë¼ì§.
    """
    if config['training']['do_train']:
        print("\n=== ëª¨ë¸ í•™ìŠµ ì‹œì‘ (do_train: True) ===")
        device: torch.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
        print(f'---------- device : {device} ----------')

        # 1. í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ (PEP 8ì— ë”°ë¼ ë³€ìˆ˜ëª… ìˆ˜ì •)
        model: AutoModelForSeq2SeqLM
        tokenizer: AutoTokenizer
        model, tokenizer = load_tokenizer_and_model_for_train(config, device)

        # 2. ë°ì´í„° ì „ì²˜ë¦¬ê¸° ì¤€ë¹„
        bos_token: str = tokenizer.bos_token if tokenizer.bos_token else tokenizer.pad_token
        eos_token: str = tokenizer.eos_token if tokenizer.eos_token else "</s>"
        preprocessor: Preprocess = Preprocess(bos_token, eos_token)

        # 3. ë°ì´í„°ì…‹ ì¤€ë¹„
        data_path: str = config['general']['data_path']
        train_inputs_dataset: DatasetForTrain
        val_inputs_dataset: DatasetForVal
        train_inputs_dataset, val_inputs_dataset = prepare_train_dataset(
            config, preprocessor, data_path, tokenizer
        )

        # 4. Trainer ë¡œë“œ ë° í•™ìŠµ ì‹œì‘
        trainer: Seq2SeqTrainer = load_trainer_for_train(
            config, model, tokenizer, train_inputs_dataset, val_inputs_dataset
        )
        trainer.train()
        print("\n=== ëª¨ë¸ í•™ìŠµ ì™„ë£Œ ===")
    else:
        print("\n=== ì¶”ë¡  ëª¨ë“œ (do_train: False) ===\n")
    

# ê²°ê³¼ ì˜ˆìƒì ìˆ˜ ì²´í¬í•˜ê¸°
# PEP 8 ì¤€ìˆ˜: í•¨ìˆ˜ëª…ì€ ì†Œë¬¸ì ìŠ¤ë„¤ì´í¬ ì¼€ì´ìŠ¤
def calculate_local_rouge(predicted_df: pd.DataFrame, dev_path: str) -> None:
    """
    ì˜ˆì¸¡ëœ ìš”ì•½ê³¼ dev.csvì˜ ì •ë‹µì„ ë¹„êµí•˜ì—¬ ROUGE-1/2/L F1 ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        predicted_df (pd.DataFrame): inference ê²°ê³¼ DataFrame (fname, summary ì»¬ëŸ¼ í¬í•¨).
        dev_path (str): dev.csv íŒŒì¼ ê²½ë¡œ.
    """
    # 1. ì •ë‹µ ë°ì´í„°(dev.csv) ë¡œë“œ ë° ì •ë¦¬
    if not os.path.exists(dev_path):
        print(f"\nâŒ [ERROR] Dev íŒŒì¼ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {dev_path}")
        return

    dev_df: pd.DataFrame = pd.read_csv(dev_path)
    # ì •ë‹µ íŒŒì¼ì˜ fnameê³¼ summaryë§Œ ì‚¬ìš© (ëŒ€íšŒ ë°ì´í„° êµ¬ì¡° ê¸°ì¤€)
    reference_df: pd.DataFrame = dev_df[['fname', 'summary']].rename(columns={'summary': 'reference_summary'})
    
    # 2. ì˜ˆì¸¡ ê²°ê³¼ì™€ ì •ë‹µ ë°ì´í„° ë³‘í•© (fname ê¸°ì¤€ìœ¼ë¡œ)
    # [í•µì‹¬] ì˜ˆì¸¡ ê²°ê³¼ì— test_xxxê°€ ìˆìœ¼ë©´ dev_xxxì™€ ë³‘í•©ë˜ì§€ ì•ŠìŒ.
    merged_df: pd.DataFrame = pd.merge(predicted_df, reference_df, on='fname', how='inner')

    # ë³‘í•© í›„ í–‰ì´ ì—†ë‹¤ë©´, í˜„ì¬ outputì´ test setì— ëŒ€í•œ ì˜ˆì¸¡ ê²°ê³¼ì„ì„ ì˜ë¯¸
    if merged_df.empty:
        print("\nâš ï¸ [ê²½ê³ ] ë¡œì»¬ ROUGE ê³„ì‚° ì‹¤íŒ¨: ì˜ˆì¸¡ ê²°ê³¼ì— dev ë°ì´í„°ê°€ ì—†ì–´ ì •ë‹µê³¼ ë³‘í•©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("         - í˜„ì¬ ì¶”ë¡  ê²°ê³¼ëŠ” 'test.csv'ì— ëŒ€í•œ ê²ƒì…ë‹ˆë‹¤. ë¡œì»¬ ê²€ì¦ì€ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
    
    # 3. ROUGE Scorer ì´ˆê¸°í™” (ëŒ€íšŒ í‰ê°€ ì§€í‘œ: ROUGE-1, ROUGE-2, ROUGE-Lì˜ F1 score)
    # 'rouge_scorer' ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©: 'rouge-score' íŒ¨í‚¤ì§€ë¥¼ í†µí•´ ì„¤ì¹˜ë¨
    scorer: rouge_scorer.RougeScorer = rouge_scorer.RougeScorer(
        ['rouge1', 'rouge2', 'rougeL'], 
        use_stemmer=False  
    )

    all_rouge1: List[float] = []
    all_rouge2: List[float] = []
    all_rougel: List[float] = []

    # 4. ê° ëŒ€í™”ë³„ë¡œ ROUGE ì ìˆ˜ ê³„ì‚°
    for _, row in merged_df.iterrows(): # ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì„ ë•Œ _ ì‚¬ìš© (PEP 8)
        # ì˜ˆì¸¡ ìš”ì•½ (Candidate)
        candidate: str = str(row['summary'])  
        # ì •ë‹µ ìš”ì•½ (Reference)
        reference: str = str(row['reference_summary'])  
        
        # ROUGE ì ìˆ˜ ê³„ì‚°
        # score í•¨ìˆ˜ì˜ ì¸ìëŠ” (reference, candidate) ìˆœì„œì„
        scores: Dict[str, rouge_scorer.Score] = scorer.score(reference, candidate)
        
        # F1-scoreë§Œ ì¶”ì¶œ
        all_rouge1.append(scores['rouge1'].fmeasure)
        all_rouge2.append(scores['rouge2'].fmeasure)
        all_rougel.append(scores['rougeL'].fmeasure)

    # ì—¬ê¸°ì— ì•ˆì „ ì¥ì¹˜ ì¶”ê°€
    if not all_rouge1:
        print("ê²½ê³ : Rouge ì ìˆ˜ë¥¼ ê³„ì‚°í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (all_rouge1 ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆìŒ)")
        return  # ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìœ¼ë©´ í•¨ìˆ˜ë¥¼ ì¢…ë£Œ


    # ì„¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¶ì–´ì„œ í‰ê·  ê³„ì‚°
    rouge_lists: Dict[str, List[float]] = {
        'rouge_1': all_rouge1,
        'rouge_2': all_rouge2,
        'rouge_l': all_rougel
    }
    
    avg_scores: Dict[str, float] = {}
    
    for name, scores in rouge_lists.items():
        avg_scores[name] = sum(scores) / len(scores)

    # ìµœì¢… í‰ê·  ê³„ì‚°
    final_avg_f1_score: float = sum(avg_scores.values()) / len(avg_scores)

    print("\n" + "="*50)
    print("âœ¨âœ¨âœ¨ ë¡œì»¬ ê²€ì¦ (Dev Set) ROUGE F1 ì ìˆ˜ âœ¨âœ¨âœ¨")
    print(f"ROUGE-1 F1 í‰ê· : {avg_scores['rouge_1'] * 100:.4f}")
    print(f"ROUGE-2 F1 í‰ê· : {avg_scores['rouge_2'] * 100:.4f}")
    print(f"ROUGE-L F1 í‰ê· : {avg_scores['rouge_l'] * 100:.4f}")
    print(f"â¡ï¸ ìµœì¢… ë¡œì»¬ í‰ê·  F1 ì ìˆ˜: {final_avg_f1_score * 100:.4f}")
    print("="*50 + "\n")


# if __name__ == "__main__":
#     # training/do_train: Trueì¸ ê²½ìš° main í•¨ìˆ˜ ë‚´ì—ì„œ í•™ìŠµì´ ì§„í–‰ë¨
#     if loaded_config['training']['do_train']:
#         main(loaded_config)
    
#     # ì¶”ë¡  ì „ ìµœì‹  ì²´í¬í¬ì¸íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ë¡œë“œí•˜ë„ë¡ configë¥¼ ì—…ë°ì´íŠ¸ (í•™ìŠµì„ í–ˆì„ ê²½ìš°ì—ë§Œ ì˜ë¯¸ ìˆìŒ)
#     update_inference_path_with_latest_checkpoint(loaded_config)

#     # config.yamlì˜ ckt_pathë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ìˆ˜ë™ìœ¼ë¡œ checkpoint ê°•ì œ ì§€ì •
#     loaded_config['inference']['ckt_path'] = './checkpoint-10123'
#     print(f"\nâœ… [ìµœì¢… ê°•ì œ ì„¤ì •] ì¶”ë¡  ì²´í¬í¬ì¸íŠ¸: '{loaded_config['inference']['ckt_path']}'")
    
#     # [í•µì‹¬ ìˆ˜ì •] is_final_submission=Trueë¡œ ë³€ê²½í•˜ì—¬ test.csvì— ëŒ€í•œ ìµœì¢… ì œì¶œ íŒŒì¼ì„ ìƒì„±í•¨
#     # is_final_submission=Trueì´ë©´ test.csvë¥¼ ë¡œë“œí•˜ê³ , prediction.csvë¡œ ì €ì¥ë¨
#     print("\nâ­ â­ â­ ìµœì¢… ì œì¶œ íŒŒì¼ ìƒì„± ëª¨ë“œ: test.csvë¡œ ì¶”ë¡ ì„ ì‹œì‘í•©ë‹ˆë‹¤! â­ â­ â­")
    
#     # PEP 8 ì¤€ìˆ˜ë¥¼ ìœ„í•´ ë³€ìˆ˜ëª…ì— _output ëŒ€ì‹  output ì‚¬ìš©
#     output: pd.DataFrame = inference(loaded_config, is_final_submission=True)

#     print(output.head())
#     print(f"\nâœ… ìµœì¢… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {loaded_config['inference']['result_path']}/prediction.csv")

#     # dev_data_path: str = os.path.join(loaded_config['general']['data_path'], 'dev.csv')
#     # calculate_local_rouge(output, dev_data_path) # test.csv ì¶”ë¡  ì‹œ ë¡œì»¬ ê²€ì¦ì€ ìŠ¤í‚µ

# if __name__ == "__main__":
#     # ----------------------------------------------------------------------
#     # ğŸ”¥ [ë‹¨ì¶• ì‹¤í–‰] í›ˆë ¨ì„ ê±´ë„ˆë›°ê³ , ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ë¡œ ë°”ë¡œ ì¶”ë¡ ì„ ì‹œì‘í•©ë‹ˆë‹¤.
#     # ----------------------------------------------------------------------
#     loaded_config['training']['do_train'] = False # í›ˆë ¨ ê±´ë„ˆë›°ê¸°
    
#     # ğŸ’¡ [ìµœê³ ì  ë³µì›] ckt_pathë¥¼ ìµœê³ ì  ë•Œì˜ ì²´í¬í¬ì¸íŠ¸ì¸ './checkpoint-10123'ë¡œ ê°•ì œ ì§€ì •
#     loaded_config['inference']['ckt_path'] = './checkpoint-10123'
    
#     # â­ [íŒ€ì› ì„¤ì • ì ìš©] ì¶”ë¡  íŒŒë¼ë¯¸í„°ë¥¼ íŒ€ì›ì˜ ìš°ìˆ˜ ì„¤ì •ìœ¼ë¡œ ë³€ê²½
#     loaded_config['inference']['generate_max_length'] = 90  # ë„¤ ê¸°ì¡´ 90 ìœ ì§€
#     loaded_config['inference']['num_beams'] = 6              # 4 â†’ 6
#     loaded_config['inference']['no_repeat_ngram_size'] = 3   # 2 â†’ 3
#     loaded_config['inference']['length_penalty'] = 1.2       # ì¶”ê°€
#     loaded_config['inference']['min_length'] = 20            # ì¶”ê°€
#     loaded_config['inference']['repetition_penalty'] = 1.2   # ì¶”ê°€
    
#     print(f"âœ… [ìµœê³ ì  ë³µì›] ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ: {loaded_config['inference']['ckt_path']}")
#     print(f"âœ… [íŒ€ì› ì„¤ì • ì ìš©] Num Beams: {loaded_config['inference']['num_beams']}, No Repeat Ngram: {loaded_config['inference']['no_repeat_ngram_size']}")
    
#     # [í•µì‹¬] is_final_submission=Trueë¡œ test.csvì— ëŒ€í•œ ìµœì¢… ì œì¶œ íŒŒì¼ì„ ìƒì„±í•¨
#     print("\nâ­ â­ â­ íŒ€ì› ì„¤ì •ìœ¼ë¡œ test.csv ì¶”ë¡  ì‹œì‘! â­ â­ â­")
    
#     # inference í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ 'test.csv' íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡ ì´ ì§„í–‰ë¨
#     output: pd.DataFrame = inference(loaded_config, is_final_submission=True)

#     print(output.head())
#     output_filename: str = "prediction_final.csv" # íŒŒì¼ ì´ë¦„ ë°”ê¿”ì„œ ì €ì¥í•´!
#     print(f"\nâœ… ìµœì¢… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {loaded_config['inference']['result_path']}/{output_filename}")

if __name__ == "__main__":
    # ----------------------------------------------------------------------
    # ğŸ”¥ [ë‹¨ì¶• ì‹¤í–‰] í›ˆë ¨ì„ ê±´ë„ˆë›°ê³ , ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ë¡œ ë°”ë¡œ ì¶”ë¡ ì„ ì‹œì‘í•©ë‹ˆë‹¤.
    # ----------------------------------------------------------------------
    loaded_config['training']['do_train'] = False # í›ˆë ¨ ê±´ë„ˆë›°ê¸°
    
    # ğŸ’¡ [ìµœê³ ì  ë³µì›] ckt_pathë¥¼ ìµœê³ ì  ë•Œì˜ ì²´í¬í¬ì¸íŠ¸ì¸ './checkpoint-10123'ë¡œ ê°•ì œ ì§€ì •
    loaded_config['inference']['ckt_path'] = './checkpoint-10123/checkpoint-10123'
    
    # â­ [1ë‹¨ê³„ ì‹¤í—˜] no_repeat_ngram_sizeë§Œ 3ìœ¼ë¡œ ë³€ê²½
    loaded_config['inference']['generate_max_length'] = 90
    loaded_config['inference']['num_beams'] = 4 # (ê¸°ì¡´ 4 ìœ ì§€)
    loaded_config['inference']['no_repeat_ngram_size'] = 3  # 2 â†’ 3 ğŸ”¥
    loaded_config['inference']['length_penalty'] = 1.0  # (ê¸°ë³¸ê°’ 1.0 ìœ ì§€)
    loaded_config['inference']['min_length'] = 0  # (ê¸°ë³¸ê°’ 0 ìœ ì§€)
    loaded_config['inference']['repetition_penalty'] = 1.0  # (ê¸°ë³¸ê°’ 1.0 ìœ ì§€)
    
    print(f"âœ… [ìµœê³ ì  ë³µì›] ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ: {loaded_config['inference']['ckt_path']}")
    print(f"âœ… [íŒ€ì› ì„¤ì • ì ìš©] Num Beams: {loaded_config['inference']['num_beams']}, No Repeat Ngram: {loaded_config['inference']['no_repeat_ngram_size']}")
    
    # ğŸ’¡ ë””ë²„ê¹… ì½”ë“œ ì¶”ê°€: ì—¬ê¸°ì„œ ì„¤ì •ì´ 3ìœ¼ë¡œ ë°”ë€Œì—ˆëŠ”ì§€ í•œ ë²ˆ ë” í™•ì¸
    print(f"\n[ë””ë²„ê·¸] inference ì„¤ì •: {loaded_config['inference']}") 
    
    # [í•µì‹¬] is_final_submission=Trueë¡œ test.csvì— ëŒ€í•œ ìµœì¢… ì œì¶œ íŒŒì¼ì„ ìƒì„±í•¨
    print("\nâ­ â­ â­ 1ë‹¨ê³„ ì‹¤í—˜ (no_repeat_ngram_size=3) ì¶”ë¡  ì‹œì‘! â­ â­ â­")
    
    # inference í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ 'test.csv' íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡ ì´ ì§„í–‰ë¨
    # ì´ì œ ì—¬ê¸°ì„œ loaded_configë¥¼ ë„˜ê¸¸ ë•Œ 'no_repeat_ngram_size': 3ì´ í™•ì‹¤íˆ ì „ë‹¬
    output: pd.DataFrame = inference(loaded_config, is_final_submission=True)

    print(output.head())
    output_filename: str = "prediction_step1.csv" # íŒŒì¼ ì´ë¦„ì„ 1ë‹¨ê³„ìš©ìœ¼ë¡œ ë°”ê¿”ì„œ ì €ì¥
    print(f"\nâœ… ìµœì¢… ì œì¶œ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {loaded_config['inference']['result_path']}/{output_filename}")