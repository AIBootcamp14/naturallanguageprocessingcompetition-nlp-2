import os
import re
import json
import yaml
from glob import glob
from pprint import pprint
from typing import List, Dict, Tuple, Any, Optional, Union

import pandas as pd
import torch
import pytorch_lightning as pl
from tqdm import tqdm
from rouge import Rouge
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer,
    EarlyStoppingCallback
)
import evaluate
import wandb
from transformers.modeling_utils import PreTrainedModel


# --------------------------------------------------------------------------------------
# ì „ì—­ ì„¤ì • ë° PEP 8 ì¤€ìˆ˜
# --------------------------------------------------------------------------------------
CONFIG_PATH: str = "./config.yaml"


# --------------------------------------------------------------------------------------
# config.yaml íŒŒì¼ì—ì„œ ì„¤ì •ê°’ì„ ì§ì ‘ ë¶ˆëŸ¬ì˜¤ë„ë¡ ë³€ê²½
# --------------------------------------------------------------------------------------
try:
    with open(CONFIG_PATH, "r", encoding="utf-8") as file:
        loaded_config: Dict[str, Any] = yaml.safe_load(file)
except FileNotFoundError:
    print(f"ì˜¤ë¥˜: ì„¤ì • íŒŒì¼ '{CONFIG_PATH}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. config.yamlì„ ë¨¼ì € ìƒì„±í•˜ì„¸ìš”.")
    exit()

# ë¶ˆëŸ¬ì˜¨ config ë‚´ìš©ì„ ì¶œë ¥
pprint(loaded_config)
print(loaded_config.get('general'))
print(loaded_config.get('tokenizer'))
print(loaded_config.get('training'))
print(loaded_config.get('wandb'))
print(loaded_config.get('inference'))

data_path: str = loaded_config['general']['data_path']

# train dataì™€ validation data ë¶ˆëŸ¬ì˜¤ê¸°
try:
    train_df: pd.DataFrame = pd.read_csv(os.path.join(data_path, 'train.csv'))
    print(train_df.tail())
    val_df: pd.DataFrame = pd.read_csv(os.path.join(data_path, 'dev.csv'))
    print(val_df.tail())
except FileNotFoundError as e:
    print(f"ë°ì´í„° íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {e}. 'data_path' ì„¤ì •ì„ í™•ì¸í•´.")


class Preprocess:
    """
    ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ í´ë˜ìŠ¤. ë°ì´í„°ì…‹ì„ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜í•˜ê³ 
    ì¸ì½”ë”(dialogue)ì™€ ë””ì½”ë”(summary)ì˜ ì…ë ¥ì„ ìƒì„±.
    """
    def __init__(
        self,
        bos_token: str,
        eos_token: str,
    ) -> None:
        self.bos_token: str = bos_token
        self.eos_token: str = eos_token

    @staticmethod
    def make_set_as_df(file_path: str, is_train: bool = True) -> pd.DataFrame:
        """ì‹¤í—˜ì— í•„ìš”í•œ ì»¬ëŸ¼(fname, dialogue, summary)ì„ ê°€ì§„ ë°ì´í„°í”„ë ˆì„ì„ ìƒì„±."""
        df: pd.DataFrame = pd.read_csv(file_path)
        if is_train:
            # PEP 8: ë¦¬í„´í•˜ëŠ” ë”•ì…”ë„ˆë¦¬/ë¦¬ìŠ¤íŠ¸ì— ê³µë°± ì¶”ê°€
            return df[['fname', 'dialogue', 'summary', 'topic']]

        return df[['fname', 'dialogue']]

    # PEP 484: Union ëŒ€ì‹  ' | ' ì‚¬ìš© (Python 3.10+ ê¸°ì¤€)
    def make_input(
        self,
        dataset: pd.DataFrame,
        is_test: bool = False
    ) -> Tuple[List[str], List[str]] | Tuple[List[str], List[str], List[str]]:
        """BART/T5 ëª¨ë¸ì˜ ì…ë ¥ í˜•íƒœë¥¼ ë§ì¶”ê¸° ìœ„í•´ ì „ì²˜ë¦¬ë¥¼ ì§„í–‰."""
        # \n ë° <br> ê°™ì€ ì¤„ ë°”ê¿ˆ ë…¸ì´ì¦ˆë¥¼ ê³µë°±ìœ¼ë¡œ í†µì¼
        cleaned_dialogues: pd.Series = dataset['dialogue'].apply(
            lambda x: str(x).replace('\n', ' ').replace('<br>', ' ')
        )

        if is_test:
            encoder_input: List[str] = cleaned_dialogues.tolist()
            # í…ŒìŠ¤íŠ¸ ì‹œì—ëŠ” ì‹¤ì œ ìš”ì•½ ëŒ€ì‹  BOS í† í°ë§Œ ë””ì½”ë” ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
            decoder_input: List[str] = [self.bos_token] * len(cleaned_dialogues)
            return encoder_input, decoder_input
        else:
            encoder_input: List[str] = cleaned_dialogues.tolist()
            # Ground truthë¥¼ ë””ì½”ë”ì˜ inputìœ¼ë¡œ ì‚¬ìš© (BOS í† í° ì¶”ê°€)
            # PEP 8: ì¤„ ë°”ê¿ˆì— ì£¼ì˜
            decoder_input: List[str] = dataset['summary'].apply(
                lambda x: self.bos_token + str(x)
            ).tolist()
            # Ground truthë¥¼ ë ˆì´ë¸”ë¡œ ì‚¬ìš© (EOS í† í° ì¶”ê°€)
            # PEP 8: ì¤„ ë°”ê¿ˆì— ì£¼ì˜
            decoder_output: List[str] = dataset['summary'].apply(
                lambda x: str(x) + self.eos_token
            ).tolist()
            return encoder_input, decoder_input, decoder_output


class DatasetForTrain(Dataset):
    """ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©ë˜ëŠ” Dataset í´ë˜ìŠ¤."""
    def __init__(
        self,
        encoder_input: Dict[str, torch.Tensor],
        decoder_input: Dict[str, torch.Tensor],
        labels: Dict[str, torch.Tensor],
        length: int
    ) -> None:
        self.encoder_input: Dict[str, torch.Tensor] = encoder_input
        self.decoder_input: Dict[str, torch.Tensor] = decoder_input
        self.labels: Dict[str, torch.Tensor] = labels
        self._length: int = length

    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        # PEP 8: ë”•ì…”ë„ˆë¦¬ ì»´í”„ë¦¬í—¨ì…˜ '{key: val[idx] ...}'ì—ì„œ ':' ë’¤ì— ê³µë°± í•˜ë‚˜
        item: Dict[str, torch.Tensor] = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}
        item_decoder: Dict[str, torch.Tensor] = {key: val[idx].clone().detach() for key, val in self.decoder_input.items()}

        item_decoder['decoder_input_ids'] = item_decoder['input_ids']
        item_decoder['decoder_attention_mask'] = item_decoder['attention_mask']
        item_decoder.pop('input_ids')
        # 'attention_ids' í‚¤ëŠ” ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì œê±° ì‹œ ì˜¤ë¥˜ ë°œìƒ ë°©ì§€ (popì— default ê°’ ì‚¬ìš©)
        item_decoder.pop('attention_ids', None)
        item_decoder.pop('attention_mask')
        
        item.update(item_decoder)
        item['labels'] = self.labels['input_ids'][idx].clone().detach()
        return item

    def __len__(self) -> int:
        return self._length


# Validationì— ì‚¬ìš©ë˜ëŠ” Dataset í´ë˜ìŠ¤ëŠ” Trainê³¼ ë™ì¼í•œ êµ¬ì¡°ë¥¼ ê°€ì§‘ë‹ˆë‹¤.
class DatasetForVal(DatasetForTrain):
    """ëª¨ë¸ ê²€ì¦ì— ì‚¬ìš©ë˜ëŠ” Dataset í´ë˜ìŠ¤."""
    pass


class DatasetForInference(Dataset):
    """ëª¨ë¸ ì¶”ë¡ ì— ì‚¬ìš©ë˜ëŠ” Dataset í´ë˜ìŠ¤."""
    def __init__(self, encoder_input: Dict[str, torch.Tensor], test_id: pd.Series, length: int) -> None:
        self.encoder_input: Dict[str, torch.Tensor] = encoder_input
        self.test_id: pd.Series = test_id
        self._length: int = length

    def __getitem__(self, idx: int) -> Dict[str, Any]:
        # PEP 8: ë”•ì…”ë„ˆë¦¬ ì»´í”„ë¦¬í—¨ì…˜ '{key: val[idx] ...}'ì—ì„œ ':' ë’¤ì— ê³µë°± í•˜ë‚˜
        item: Dict[str, torch.Tensor] = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}
        # IDëŠ” ë¬¸ìì—´ì´ë¯€ë¡œ í…ì„œê°€ ì•„ë‹Œ ê·¸ëŒ€ë¡œ ë°˜í™˜
        item['ID'] = self.test_id.iloc[idx]
        return item

    def __len__(self) -> int:
        return self._length


def prepare_train_dataset(
    config: Dict[str, Any],
    preprocessor: Preprocess,
    data_path: str,
    tokenizer: AutoTokenizer
) -> Tuple[DatasetForTrain, DatasetForVal]:
    """í›ˆë ¨ ë° ê²€ì¦ ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ê³  í† í°í™”í•˜ì—¬ Dataset ê°ì²´ë¡œ ë°˜í™˜."""
    train_file_path: str = os.path.join(data_path, 'train.csv')
    val_file_path: str = os.path.join(data_path, 'dev.csv')

    train_data: pd.DataFrame = preprocessor.make_set_as_df(train_file_path)
    val_data: pd.DataFrame = preprocessor.make_set_as_df(val_file_path)

    print('-' * 150)
    print(f'train_data:\n{train_data["dialogue"].iloc[0]}')
    print(f'train_label:\n{train_data["summary"].iloc[0]}')

    print('-' * 150)
    print(f'val_data:\n{val_data["dialogue"].iloc[0]}')
    print(f'val_label:\n{val_data["summary"].iloc[0]}')

    encoder_input_train, decoder_input_train, decoder_output_train = preprocessor.make_input(train_data)
    encoder_input_val, decoder_input_val, decoder_output_val = preprocessor.make_input(val_data)
    print('-' * 10, 'Load data complete', '-' * 10)

    tokenizer_config: Dict[str, Any] = config['tokenizer']

    # í† í¬ë‚˜ì´ì§• (í›ˆë ¨ ë°ì´í„°)
    tokenized_encoder_inputs: Dict[str, torch.Tensor] = tokenizer(
        encoder_input_train, return_tensors="pt", padding=True,
        add_special_tokens=True, truncation=True,
        max_length=tokenizer_config['encoder_max_len'],
        return_token_type_ids=False
    )
    tokenized_decoder_inputs: Dict[str, torch.Tensor] = tokenizer(
        decoder_input_train, return_tensors="pt", padding=True,
        add_special_tokens=True, truncation=True,
        max_length=tokenizer_config['decoder_max_len'],
        return_token_type_ids=False
    )
    tokenized_decoder_ouputs: Dict[str, torch.Tensor] = tokenizer(
        decoder_output_train, return_tensors="pt", padding=True,
        add_special_tokens=True, truncation=True,
        max_length=tokenizer_config['decoder_max_len'],
        return_token_type_ids=False
    )

    train_inputs_dataset: DatasetForTrain = DatasetForTrain(
        tokenized_encoder_inputs, tokenized_decoder_inputs,
        tokenized_decoder_ouputs, len(encoder_input_train)
    )

    # í† í¬ë‚˜ì´ì§• (ê²€ì¦ ë°ì´í„°)
    val_tokenized_encoder_inputs: Dict[str, torch.Tensor] = tokenizer(
        encoder_input_val, return_tensors="pt", padding=True,
        add_special_tokens=True, truncation=True,
        max_length=tokenizer_config['encoder_max_len'],
        return_token_type_ids=False
    )
    val_tokenized_decoder_inputs: Dict[str, torch.Tensor] = tokenizer(
        decoder_input_val, return_tensors="pt", padding=True,
        add_special_tokens=True, truncation=True,
        max_length=tokenizer_config['decoder_max_len'],
        return_token_type_ids=False
    )
    val_tokenized_decoder_ouputs: Dict[str, torch.Tensor] = tokenizer(
        decoder_output_val, return_tensors="pt", padding=True,
        add_special_tokens=True, truncation=True,
        max_length=tokenizer_config['decoder_max_len'],
        return_token_type_ids=False
    )

    val_inputs_dataset: DatasetForVal = DatasetForVal(
        val_tokenized_encoder_inputs, val_tokenized_decoder_inputs,
        val_tokenized_decoder_ouputs, len(encoder_input_val)
    )

    print('-' * 10, 'Make dataset complete', '-' * 10)
    return train_inputs_dataset, val_inputs_dataset


def compute_metrics(config: Dict[str, Any], tokenizer: AutoTokenizer, pred: Any) -> Dict[str, float]:
    """
    ëª¨ë¸ ì˜ˆì¸¡ ê²°ê³¼(pred)ë¥¼ ë°›ì•„ ROUGE ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ì—¬ ë°˜í™˜.
    """
    try:
        rouge_scorer: Rouge = Rouge()

        predictions: Any = pred.predictions
        labels: Any = pred.label_ids

        # -100 (ignore_index)ì„ ì‹¤ì œ íŒ¨ë”© í† í° IDë¡œ ëŒ€ì²´
        predictions[predictions == -100] = tokenizer.pad_token_id
        labels[labels == -100] = tokenizer.pad_token_id

        decoded_preds: List[str] = tokenizer.batch_decode(predictions, clean_up_tokenization_spaces=True)
        decoded_labels: List[str] = tokenizer.batch_decode(labels, clean_up_tokenization_spaces=True)

        # ë¶ˆí•„ìš”í•œ ìƒì„±í† í° ì œê±°
        replaced_predictions: List[str] = decoded_preds.copy()
        replaced_labels: List[str] = decoded_labels.copy()
        remove_tokens: List[str] = config['inference']['remove_tokens']

        for token in remove_tokens:
            replaced_predictions = [sentence.replace(token, " ").strip() for sentence in replaced_predictions]
            replaced_labels = [sentence.replace(token, " ").strip() for sentence in replaced_labels]

        # ë¡œê·¸ ì¶œë ¥ (ì²« 3ê°œ ìƒ˜í”Œ)
        print('-' * 150)
        print(f"PRED: {replaced_predictions[0]}")
        print(f"GOLD: {replaced_labels[0]}")
        print('-' * 150)
        print(f"PRED: {replaced_predictions[1]}")
        print(f"GOLD: {replaced_labels[1]}")
        print('-' * 150)
        print(f"PRED: {replaced_predictions[2]}")
        print(f"GOLD: {replaced_labels[2]}")
        print('-' * 150)

        # ìµœì¢…ì ì¸ ROUGE ì ìˆ˜ë¥¼ ê³„ì‚°
        # ROUGE ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ë¹ˆ ë¬¸ìì—´ì—ì„œ ì˜¤ë¥˜ê°€ ë‚  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë¹ˆ ë¬¸ìì—´ì€ ì œê±°
        valid_preds: List[str] = [p for p in replaced_predictions if p.strip()]
        valid_labels: List[str] = [l for p, l in zip(replaced_predictions, replaced_labels) if p.strip()]

        if not valid_preds:
            print("ê²½ê³ : ìœ íš¨í•œ ì˜ˆì¸¡ ë¬¸ìì—´ì´ ì—†ìŠµë‹ˆë‹¤.")
            return {}

        # get_scoresì˜ ì…ë ¥ìœ¼ë¡œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ê°€ ë“¤ì–´ê°€ì§€ ì•Šë„ë¡ ìœ íš¨ì„± ê²€ì‚¬
        if not valid_preds or not valid_labels:
            print("ê²½ê³ : ìœ íš¨í•œ ì˜ˆì¸¡ ë˜ëŠ” ë ˆì´ë¸”ì´ ì—†ì–´ ROUGE ì ìˆ˜ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {}

        results: Dict[str, Dict[str, float]] = rouge_scorer.get_scores(valid_preds, valid_labels, avg=True)

        # ROUGE ì ìˆ˜ ì¤‘ F-1 scoreë¥¼ í†µí•´ í‰ê°€
        result: Dict[str, float] = {key: value["f"] for key, value in results.items()}

        # ì†Œìˆ˜ì  4ìë¦¬ê¹Œì§€ ë°˜ì˜¬ë¦¼
        result = {k: round(v * 100, 4) for k, v in result.items()}

        return result

    except Exception as e:
        print(f"Error computing metrics: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•˜ì—¬ Trainerê°€ ë©ˆì¶”ì§€ ì•Šë„ë¡ í•¨
        return {}


def load_trainer_for_train(
    config: Dict[str, Any],
    generate_model: PreTrainedModel,
    tokenizer: AutoTokenizer,
    train_inputs_dataset: DatasetForTrain,
    val_inputs_dataset: DatasetForVal
) -> Seq2SeqTrainer:
    """Seq2SeqTrainer ê°ì²´ë¥¼ ì´ˆê¸°í™”í•˜ê³  ë°˜í™˜."""
    print('-' * 10, 'Make training arguments', '-' * 10)

    # configì˜ ê°’ì„ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ Seq2SeqTrainingArguments ìƒì„±
    training_args: Seq2SeqTrainingArguments = Seq2SeqTrainingArguments(
        output_dir=config['general']['output_dir'],
        overwrite_output_dir=config['training']['overwrite_output_dir'],
        num_train_epochs=config['training']['num_train_epochs'],
        learning_rate=config['training']['learning_rate'],
        per_device_train_batch_size=config['training']['per_device_train_batch_size'],
        per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],
        warmup_ratio=config['training']['warmup_ratio'],
        weight_decay=config['training']['weight_decay'],
        lr_scheduler_type=config['training']['lr_scheduler_type'],
        optim=config['training']['optim'],
        gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],
        evaluation_strategy=config['training']['evaluation_strategy'],
        save_strategy=config['training']['save_strategy'],
        save_total_limit=config['training']['save_total_limit'],
        fp16=config['training']['fp16'],
        load_best_model_at_end=config['training']['load_best_model_at_end'],
        seed=config['training']['seed'],
        logging_dir=config['training']['logging_dir'],
        logging_strategy=config['training']['logging_strategy'],
        predict_with_generate=config['training']['predict_with_generate'],
        generation_max_length=config['training']['generation_max_length'],
        do_train=config['training']['do_train'],
        do_eval=config['training']['do_eval'],
        report_to=config['training']['report_to'],
        # ë©”ëª¨ë¦¬ ì ˆì•½ ê¸°ëŠ¥ í™œì„±í™” (config.yamlì—ì„œ ê°’ì„ ê°€ì ¸ì˜´)
        gradient_checkpointing=config['training'].get('gradient_checkpointing', False),
        # PEP 8: eval_strategyë¡œ ë³€ê²½ (ë²„ì „ì—…ì— ëŒ€ë¹„)
        eval_strategy=config['training']['evaluation_strategy'], 
    )

    # Validation lossê°€ ë” ì´ìƒ ê°œì„ ë˜ì§€ ì•Šì„ ë•Œ í•™ìŠµì„ ì¤‘ë‹¨ì‹œí‚¤ëŠ” EarlyStopping ê¸°ëŠ¥
    my_callback: EarlyStoppingCallback = EarlyStoppingCallback(
        early_stopping_patience=config['training']['early_stopping_patience'],
        early_stopping_threshold=config['training']['early_stopping_threshold']
    )

    print('-' * 10, 'Make training arguments complete', '-' * 10)
    print('-' * 10, 'Make trainer', '-' * 10)

    # Trainer í´ë˜ìŠ¤ ì •ì˜
    trainer: Seq2SeqTrainer = Seq2SeqTrainer(
        model=generate_model,
        args=training_args,
        train_dataset=train_inputs_dataset,
        eval_dataset=val_inputs_dataset,
        compute_metrics=lambda pred: compute_metrics(config, tokenizer, pred),
        callbacks=[my_callback]
    )
    print('-' * 10, 'Make trainer complete', '-' * 10)

    return trainer


def load_tokenizer_and_model_for_train(
    config: Dict[str, Any],
    device: torch.device
) -> Tuple[AutoModelForSeq2SeqLM, AutoTokenizer]:
    """í•™ìŠµì„ ìœ„í•œ tokenizerì™€ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜´."""
    print('-' * 10, 'Load tokenizer & model', '-' * 10)

    model_name: str = config['general']['model_name']
    print('-' * 10, f'Model Name : {model_name}', '-' * 10)

    # configì— ì„¤ì •ëœ ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ì„ ë¡œë“œ
    local_tokenizer: AutoTokenizer = AutoTokenizer.from_pretrained(model_name)
    generate_model: AutoModelForSeq2SeqLM = AutoModelForSeq2SeqLM.from_pretrained(model_name)

    special_tokens_dict: Dict[str, List[str]] = {'additional_special_tokens': config['tokenizer']['special_tokens']}
    # ì•ˆì „í•˜ê²Œ ë‹¤ì‹œ ì¶”ê°€í•˜ê³  resize_token_embeddings í˜¸ì¶œ
    local_tokenizer.add_special_tokens(special_tokens_dict)

    # special tokenì„ ì¶”ê°€í–ˆìœ¼ë¯€ë¡œ ì„ë² ë”© ë ˆì´ì–´ í¬ê¸° ì¬ì¡°ì •
    generate_model.resize_token_embeddings(len(local_tokenizer))
    generate_model.to(device)
    print(generate_model.config)

    print('-' * 10, 'Load tokenizer & model complete', '-' * 10)
    return generate_model, local_tokenizer


def find_latest_checkpoint(output_dir: str = './') -> Optional[str]:
    """ê°€ì¥ ìµœê·¼ì— ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ í´ë” ê²½ë¡œë¥¼ ì°¾ëŠ”ë‹¤."""
    # glob ëª¨ë“ˆì„ ì‚¬ìš©í•˜ì—¬ 'checkpoint-'ë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  í´ë”ë¥¼ ì°¾ìŒ
    checkpoint_dirs: List[str] = glob(f"{output_dir}/checkpoint-*")
    
    if not checkpoint_dirs:
        return None
        
    # í´ë” ì´ë¦„ ëŒ€ì‹ , ìˆ˜ì • ì‹œê°„ì„ ê¸°ì¤€ìœ¼ë¡œ ê°€ì¥ ìµœê·¼ì˜ í´ë”ë¥¼ ì„ íƒ
    # os.path.getmtime: íŒŒì¼/í´ë”ì˜ ë§ˆì§€ë§‰ ìˆ˜ì • ì‹œê°„ì„ ì´ˆ ë‹¨ìœ„ë¡œ ë°˜í™˜
    latest_checkpoint: str = max(checkpoint_dirs, key=os.path.getmtime)
    
    return latest_checkpoint


def update_inference_path_with_latest_checkpoint(config: Dict[str, Any]) -> None:
    """í•™ìŠµ í´ë”ì—ì„œ ê°€ì¥ ìµœê·¼ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œë¥¼ ì°¾ì•„ configë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤."""
    output_dir: str = config['general']['output_dir']
    
    # load_best_model_at_endê°€ Trueì¼ ë•Œ, ê°€ì¥ ìµœì ì˜ ëª¨ë¸ì´ ë§ˆì§€ë§‰ ì²´í¬í¬ì¸íŠ¸ í´ë”ì— ì €ì¥ë¨
    latest_path: Optional[str] = find_latest_checkpoint(output_dir)

    if latest_path:
        # ì°¾ì€ ìµœì‹  ê²½ë¡œë¡œ config['inference']['ckt_path']ë¥¼ ë®ì–´ì“°ê¸°
        config['inference']['ckt_path'] = latest_path 
        print(f"\nâœ¨ [ìë™ ì„¤ì •] ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ: '{latest_path}'ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
        # ì—…ë°ì´íŠ¸ëœ ê²½ë¡œë¥¼ ì €ì¥ (ì„ íƒ ì‚¬í•­ì´ì§€ë§Œ ì•ˆì „ì„ ìœ„í•´)
        try:
            # PEP 8: íŒŒì¼ ëª¨ë“œ ì¸ìˆ˜ì— ê³µë°± ì œê±°
            with open(CONFIG_PATH, 'w', encoding='utf-8') as f:
                yaml.dump(config, f)
        except Exception as e:
            print(f"ê²½ê³ : config.yaml íŒŒì¼ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ - {e}")
    else:
        print("\nâš ï¸ [ìë™ ì„¤ì • ì‹¤íŒ¨] ì²´í¬í¬ì¸íŠ¸ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. config.yamlì˜ ckt_pathë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    

def prepare_test_dataset(
    config: Dict[str, Any],
    preprocessor: Preprocess,
    tokenizer: AutoTokenizer
) -> Tuple[pd.DataFrame, DatasetForInference]:
    """í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ê³  í† í°í™”í•˜ì—¬ Dataset ê°ì²´ë¡œ ë°˜í™˜."""
    test_file_path: str = os.path.join(config['general']['data_path'], 'test.csv')

    test_data: pd.DataFrame = preprocessor.make_set_as_df(test_file_path, is_train=False)
    test_id: pd.Series = test_data['fname']

    print('-' * 150)
    print(f'test_data:\n{test_data["dialogue"].iloc[0]}')
    print('-' * 150)

    encoder_input_test, decoder_input_test = preprocessor.make_input(test_data, is_test=True)
    print('-' * 10, 'Load data complete', '-' * 10)

    tokenizer_config: Dict[str, Any] = config['tokenizer']

    # í† í¬ë‚˜ì´ì§• (ì¸ì½”ë” ì…ë ¥)
    test_tokenized_encoder_inputs: Dict[str, torch.Tensor] = tokenizer(
        encoder_input_test, return_tensors="pt", padding=True,
        add_special_tokens=True, truncation=True,
        max_length=tokenizer_config['encoder_max_len'],
        return_token_type_ids=False
    )
    # í† í¬ë‚˜ì´ì§• (ë””ì½”ë” ì…ë ¥ - T5ì—ì„œëŠ” í•„ìš” ì—†ì§€ë§Œ êµ¬ì¡° ìœ ì§€ë¥¼ ìœ„í•´ í¬í•¨)
    _test_tokenized_decoder_inputs: Dict[str, torch.Tensor] = tokenizer(
        decoder_input_test, return_tensors="pt", padding=True,
        add_special_tokens=True, truncation=True,
        max_length=tokenizer_config['decoder_max_len'],
        return_token_type_ids=False
    )

    test_encoder_inputs_dataset: DatasetForInference = DatasetForInference(
        test_tokenized_encoder_inputs, test_id, len(encoder_input_test)
    )
    print('-' * 10, 'Make dataset complete', '-' * 10)

    return test_data, test_encoder_inputs_dataset


def load_tokenizer_and_model_for_test(
    config: Dict[str, Any],
    device: torch.device
) -> Tuple[AutoModelForSeq2SeqLM, AutoTokenizer]:
    """ì¶”ë¡ ì„ ìœ„í•œ tokenizerì™€ í•™ìŠµëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜´."""
    print('-' * 10, 'Load tokenizer & model', '-' * 10)

    model_name: str = config['general']['model_name']
    ckt_path: str = config['inference']['ckt_path']
    print('-' * 10, f'Model Name : {model_name}', '-' * 10)
    print('-' * 10, f'Checkpoint Path : {ckt_path}', '-' * 10)

    # configì— ì„¤ì •ëœ ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œ
    local_tokenizer: AutoTokenizer = AutoTokenizer.from_pretrained(model_name)
    special_tokens_dict: Dict[str, List[str]] = {'additional_special_tokens': config['tokenizer']['special_tokens']}
    local_tokenizer.add_special_tokens(special_tokens_dict)

    # ğŸ’¥ğŸ’¥ğŸ’¥ [í•µì‹¬ ìˆ˜ì •] safe_serialization=False ì˜µì…˜ì„ ì œê±°í•¨! ğŸ’¥ğŸ’¥ğŸ’¥
    generate_model: AutoModelForSeq2SeqLM = AutoModelForSeq2SeqLM.from_pretrained(
        ckt_path, 
        low_cpu_mem_usage=True
    )
    generate_model.resize_token_embeddings(len(local_tokenizer))
    generate_model.to(device)
    print('-' * 10, 'Load tokenizer & model complete', '-' * 10)

    return generate_model, local_tokenizer


def inference(config: Dict[str, Any]) -> pd.DataFrame:
    """í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë°˜í™˜."""
    device: torch.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    print('-' * 10, f'device : {device}', '-' * 10)
    print(torch.__version__)

    generate_model: AutoModelForSeq2SeqLM
    local_tokenizer: AutoTokenizer
    try:
        generate_model, local_tokenizer = load_tokenizer_and_model_for_test(config, device)
    except Exception as e:
        print(f"ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}. 'ckt_path'ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return pd.DataFrame() 

    # T5 ëª¨ë¸ì˜ BOS/EOS í† í°ì€ í† í¬ë‚˜ì´ì €ì˜ ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •
    bos_token: str = local_tokenizer.bos_token if local_tokenizer.bos_token else local_tokenizer.pad_token
    eos_token: str = local_tokenizer.eos_token if local_tokenizer.eos_token else "</s>"

    preprocessor: Preprocess = Preprocess(bos_token, eos_token)

    test_data: pd.DataFrame
    test_encoder_inputs_dataset: DatasetForInference
    try:
        test_data, test_encoder_inputs_dataset = prepare_test_dataset(
            config, preprocessor, local_tokenizer)
    except Exception as e:
        print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì¤€ë¹„ ì˜¤ë¥˜: {e}. 'data_path'ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return pd.DataFrame() 

    dataloader: DataLoader = DataLoader(
        test_encoder_inputs_dataset,
        batch_size=config['inference']['batch_size'],
        shuffle=False
    )

    summary: List[str] = []
    text_ids: List[str] = []
    generate_model.eval() 
    with torch.no_grad():
        for item in tqdm(dataloader):
            # IDëŠ” ë¬¸ìì—´(str)ì´ë¯€ë¡œ ë¦¬ìŠ¤íŠ¸ì— ë‹´ì„ ë•Œ extend
            text_ids.extend(item['ID'])

            # CUDA ì¥ì¹˜ë¡œ í…ì„œë¥¼ ì´ë™
            # PEP 8: ë”•ì…”ë„ˆë¦¬ ì ‘ê·¼ ì‹œ ê³µë°± ì œê±°
            input_ids: torch.Tensor = item['input_ids'].to(device)

            generated_ids: torch.Tensor = generate_model.generate(
                input_ids=input_ids,
                no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],
                early_stopping=config['inference']['early_stopping'],
                max_length=config['inference']['generate_max_length'],
                num_beams=config['inference']['num_beams'],
            )
            for ids in generated_ids:
                result: str = local_tokenizer.decode(ids)
                summary.append(result)

    # ì •í™•í•œ í‰ê°€ë¥¼ ìœ„í•˜ì—¬ ë…¸ì´ì¦ˆì— í•´ë‹¹ë˜ëŠ” ìŠ¤í˜ì…œ í† í°ì„ ì œê±°
    remove_tokens: List[str] = config['inference']['remove_tokens']
    preprocessed_summary: List[str] = summary.copy()
    for token in remove_tokens:
        # replace í›„ stripì„ ì¶”ê°€í•˜ì—¬ ê¹”ë”í•œ ê²°ê³¼ ë³´ì¥
        preprocessed_summary = [sentence.replace(token, " ").strip() for sentence in preprocessed_summary]

    output: pd.DataFrame = pd.DataFrame(
        {
            "fname": test_data['fname'],
            "summary": preprocessed_summary,
        }
    )
    result_path: str = config['inference']['result_path']
    if not os.path.exists(result_path):
        os.makedirs(result_path)
    output.to_csv(os.path.join(result_path, "output.csv"), index=False)

    return output


def main(config: Dict[str, Any]) -> None:
    """
    í•™ìŠµ ë˜ëŠ” ì¶”ë¡ ì„ ì‹œì‘í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜.
    config['training']['do_train'] ê°’ì— ë”°ë¼ ì‘ë™ì´ ë‹¬ë¼ì§.
    """
    if config['training']['do_train']:
        print("\n=== ëª¨ë¸ í•™ìŠµ ì‹œì‘ (do_train: True) ===")
        device: torch.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
        print('-' * 10, f'device : {device}', '-' * 10)

        # 1. í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ
        generate_model: AutoModelForSeq2SeqLM
        local_tokenizer: AutoTokenizer
        generate_model, local_tokenizer = load_tokenizer_and_model_for_train(config, device)

        # 2. ë°ì´í„° ì „ì²˜ë¦¬ê¸° ì¤€ë¹„
        bos_token: str = local_tokenizer.bos_token if local_tokenizer.bos_token else local_tokenizer.pad_token
        eos_token: str = local_tokenizer.eos_token if local_tokenizer.eos_token else "</s>"
        preprocessor: Preprocess = Preprocess(bos_token, eos_token)

        # 3. ë°ì´í„°ì…‹ ì¤€ë¹„
        data_path: str = config['general']['data_path']
        train_inputs_dataset: DatasetForTrain
        val_inputs_dataset: DatasetForVal
        train_inputs_dataset, val_inputs_dataset = prepare_train_dataset(
            config, preprocessor, data_path, local_tokenizer
        )

        # 4. Trainer ë¡œë“œ ë° í•™ìŠµ ì‹œì‘
        trainer: Seq2SeqTrainer = load_trainer_for_train(
            config, generate_model, local_tokenizer, train_inputs_dataset, val_inputs_dataset
        )
        trainer.train()
        print("\n=== ëª¨ë¸ í•™ìŠµ ì™„ë£Œ ===")
    else:
        print("\n=== ì¶”ë¡  ëª¨ë“œ (do_train: False) ===\n")
        # ì¶”ë¡  ë¡œì§ì€ __main__ ë¸”ë¡ì—ì„œ ë³„ë„ë¡œ ì²˜ë¦¬


if __name__ == "__main__":
    # training/do_train: Trueì¸ ê²½ìš° main í•¨ìˆ˜ ë‚´ì—ì„œ í•™ìŠµì´ ì§„í–‰ë¨
    if loaded_config['training']['do_train']:
        main(loaded_config)
    
    # ì¶”ë¡  ì „ ìµœì‹  ì²´í¬í¬ì¸íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ë¡œë“œí•˜ë„ë¡ configë¥¼ ì—…ë°ì´íŠ¸ (í•™ìŠµì„ í–ˆì„ ê²½ìš°ì—ë§Œ ì˜ë¯¸ ìˆìŒ)
    update_inference_path_with_latest_checkpoint(loaded_config)

    # config.yamlì˜ ckt_pathë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ , ìˆ˜ë™ìœ¼ë¡œ './checkpoint-7008'ì„ ê°•ì œ ì§€ì •
    # ì´ ë¶€ë¶„ì€ ë„¤ê°€ ì§ì ‘ ë„£ì–´ì¤€ ê°•ì œ ì„¤ì •ì´ë¯€ë¡œ ê·¸ëŒ€ë¡œ ìœ ì§€í•¨
    loaded_config['inference']['ckt_path'] = './checkpoint-7008'
    print(f"\nâœ… [ìµœì¢… ê°•ì œ ì„¤ì •] ì¶”ë¡  ì²´í¬í¬ì¸íŠ¸: '{loaded_config['inference']['ckt_path']}'")

    # ì¶”ë¡  ì‹¤í–‰ (do_trainì´ Falseì¼ ë•Œë§Œ ì‹¤ì§ˆì ì¸ ì˜ë¯¸ê°€ ìˆìŒ)
    output: pd.DataFrame = inference(loaded_config)

    print(output)