import os, torch, argparse
import pandas as pd

from src.config import load_config
from src.inference.inference import inference
from src.utils.gpu_utils import clean_gpu_memory
from src.model.model_utils import load_tokenizer_and_model_for_train
from src.data.preprocess.preprocess import Preprocess
from src.data.dataset import DatasetForTrain, DatasetForVal
from src.train.train import load_trainer_for_train, compute_metrics
from src.llm.exaone import load_and_infer_exaone, load_and_train_exaone
from src.data.dataset import DatasetForInference
from src.data.preprocess.preprocess import prepare_test_dataset, prepare_train_dataset

CONFIG_PATH = "../configs/config.yaml"


def run_train(config):
    
    clean_gpu_memory()
    
    device = torch.device('cuda:0' if torch.cuda.is_available()  else 'cpu')
    
    # -------------ìžë™ìœ¼ë¡œ ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°----------------------
    # resume_from = find_latest_checkpoint(config['general']['output_dir'])
    
    # if resume_from:
    #     print(f"ðŸ”„ ì²´í¬í¬ì¸íŠ¸ ë°œê²¬! ìž¬ê°œí•©ë‹ˆë‹¤: {resume_from}")
    #     user_input = input("ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
    #     if user_input.lower() != 'y':
    #         resume_from = None
    # -------------ìžë™ìœ¼ë¡œ ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°----------------------        
            
    generate_model , tokenizer = load_tokenizer_and_model_for_train(config,device)
    
    preprocessor = Preprocess(config['tokenizer']['bos_token'], config['tokenizer']['eos_token']) 
    
    data_path = config['general']['data_path']
    train_inputs_dataset, val_inputs_dataset = prepare_train_dataset(config,preprocessor, data_path, tokenizer)
    
    trainer = load_trainer_for_train(config, generate_model,tokenizer,train_inputs_dataset,val_inputs_dataset)
    trainer.train() 
    
    # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ì €ìž¥
    trainer.save_model(config['general']['output_dir']+"/best_model")   
    tokenizer.save_pretrained(config['general']['output_dir']+"/best_model")


    
    
def run_infer(config):
    
    clean_gpu_memory()
    
    # ckt_path ="/workspace/NLP_Dialogue_Summarization/output/model/best_model"
    ckt_path ="/workspace/NLP_Dialogue_Summarization/output/model/3.best_model_training_config"
    # ckt_name = "checkpoint-6200"
    # config['inference']['ckt_path'] = ckt_path + ckt_name
    config['inference']['ckt_path'] = ckt_path
    
    return inference(config)


def run_train_exaone(config):
    clean_gpu_memory()
    load_and_train_exaone()
    

def run_infer_exaone(config):
    clean_gpu_memory()
    load_and_infer_exaone()


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('mode', type=str, choices=["train", "infer", "exaone"], help='train or infer')
    args = parser.parse_args()
    
    config = load_config(CONFIG_PATH)
    mode = args.mode
    
    if mode == "train":
        # run_train(config)
        run_train_exaone(config)
    elif mode == "exaone":
        run_infer_exaone(config)
    else:
        run_infer(config)    
        
        
def find_invalid_topics(df, max_topic_len: int = 30, min_confidence: float = 0.7):
    """
    ê¸¸ì´ê°€ ë„ˆë¬´ ê¸´ topic ë˜ëŠ” confidenceê°€ ë‚®ì€ ë°ì´í„°ë¥¼ ì°¾ì•„ëƒ„.

    Args:
        df (pd.DataFrame): fname, dialogue, topic, topic_confidence ì»¬ëŸ¼ì„ í¬í•¨í•œ ë°ì´í„°í”„ë ˆìž„
        max_topic_len (int): topic ê¸€ìž ìˆ˜ ì œí•œ ê¸°ì¤€ (ê¸°ë³¸ 15ìž ì´ˆê³¼ ì‹œ ê²½ê³ )
        min_confidence (float): ìµœì†Œ confidence ê¸°ì¤€ (ê¸°ë³¸ 0.7 ë¯¸ë§Œ ì‹œ ê²½ê³ )

    Returns:
        pd.DataFrame: ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” í–‰ë§Œ ë°˜í™˜
    """
    # ë¬¸ìžì—´ë¡œ ì•ˆì „ ë³€í™˜
    df['topic'] = df['topic'].astype(str)
    df['topic_confidence'] = df['topic_confidence'].astype(float)

    # ì¡°ê±´ ì„¤ì •
    too_long = df['topic'].apply(lambda x: len(x) > max_topic_len)
    low_conf = df['topic_confidence'] < min_confidence

    # í•„í„°ë§
    invalid_df = df[too_long | low_conf].copy()

    print(f"âš ï¸ ì´ {len(invalid_df)}ê°œì˜ ë¹„ì •ìƒ topic íƒì§€ë¨")
    print(f" - ê¸¸ì´ ì´ˆê³¼: {(too_long).sum()}ê°œ")
    print(f" - confidence < {min_confidence}: {(low_conf).sum()}ê°œ\n")

    return invalid_df 


if __name__ == "__main__":
    main()
    # clean_gpu_memory()
    
    # =============== ì œì¶œ íŒŒì¼ ê²€ì¦! =================
    df = pd.read_csv("/workspace/NLP_Dialogue_Summarization/data/test_topic_solar.csv")
    invalid_topics = find_invalid_topics(df, max_topic_len=20, min_confidence=0.7)
    print(invalid_topics[['fname', 'topic', 'topic_confidence']].head())
    # =============== ì œì¶œ íŒŒì¼ ê²€ì¦! =================